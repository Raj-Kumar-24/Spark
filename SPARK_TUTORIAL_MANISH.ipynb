{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24be4f4f-4240-4b9c-8caa-114bc2f866b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "read_csv_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d68b6faa-93c4-478b-9c85-23ef4157063c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "/FileStore/tables/2010_summary.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b83cd6a-3af5-4ab7-ac3d-65b478bc56c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=2503095305578706#setting/sparkui/1218-141137-z6024quf/driver-2257783076775701067\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=2503095305578706#setting/sparkui/1218-141137-z6024quf/driver-2257783076775701067\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d81a2dba-a737-41d5-818b-b6497178cec4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n|                 _c0|                _c1|  _c2|\n+--------------------+-------------------+-----+\n|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n|       United States|            Romania|    1|\n|       United States|            Ireland|  264|\n|       United States|              India|   69|\n|               Egypt|      United States|   24|\n|   Equatorial Guinea|      United States|    1|\n|       United States|          Singapore|   25|\n|       United States|            Grenada|   54|\n|          Costa Rica|      United States|  477|\n|             Senegal|      United States|   29|\n|       United States|   Marshall Islands|   44|\n|              Guyana|      United States|   17|\n|       United States|       Sint Maarten|   53|\n|               Malta|      United States|    1|\n|             Bolivia|      United States|   46|\n|            Anguilla|      United States|   21|\n|Turks and Caicos ...|      United States|  136|\n|       United States|        Afghanistan|    2|\n|Saint Vincent and...|      United States|    1|\n|               Italy|      United States|  390|\n+--------------------+-------------------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "flight_df=spark.read.format(\"csv\")\\\n",
    "    .option(\"header\",\"false\")\\\n",
    "        .option(\"inferSchema\",\"false\")\\\n",
    "            .option(\"mode\",\"FAILFAST\")\\\n",
    "                .load(\"/FileStore/tables/2010_summary.csv\")\n",
    "\n",
    "flight_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a2aa5a6-65c4-4076-9caf-fd485d750436",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+--------------------+-------------------+-----+\n|       United States|            Romania|    1|\n|       United States|            Ireland|  264|\n|       United States|              India|   69|\n|               Egypt|      United States|   24|\n|   Equatorial Guinea|      United States|    1|\n|       United States|          Singapore|   25|\n|       United States|            Grenada|   54|\n|          Costa Rica|      United States|  477|\n|             Senegal|      United States|   29|\n|       United States|   Marshall Islands|   44|\n|              Guyana|      United States|   17|\n|       United States|       Sint Maarten|   53|\n|               Malta|      United States|    1|\n|             Bolivia|      United States|   46|\n|            Anguilla|      United States|   21|\n|Turks and Caicos ...|      United States|  136|\n|       United States|        Afghanistan|    2|\n|Saint Vincent and...|      United States|    1|\n|               Italy|      United States|  390|\n|       United States|             Russia|  156|\n+--------------------+-------------------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "flight_df_header=spark.read.format(\"csv\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "        .option(\"inferSchema\",\"false\")\\\n",
    "            .option(\"mode\",\"FAILFAST\")\\\n",
    "                .load(\"/FileStore/tables/2010_summary.csv\")\n",
    "\n",
    "flight_df_header.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fade70cd-4ad0-41ba-a890-001446468e85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- DEST_COUNTRY_NAME: string (nullable = true)\n |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n |-- count: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "flight_df_header.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e2165ca-4489-4dd6-9c1f-1ac8e695caf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|    1|\n|    United States|            Ireland|  264|\n|    United States|              India|   69|\n|            Egypt|      United States|   24|\n|Equatorial Guinea|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "flight_df_header_schema=spark.read.format(\"csv\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "        .option(\"inferSchema\",\"true\")\\\n",
    "            .option(\"mode\",\"FAILFAST\")\\\n",
    "                .load(\"/FileStore/tables/2010_summary.csv\")\n",
    "\n",
    "flight_df_header_schema.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4894e4f3-0653-4f24-ac5e-bf36107ab462",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- DEST_COUNTRY_NAME: string (nullable = true)\n |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n |-- count: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "flight_df_header_schema.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39a025fb-be2e-4a15-ab3d-a53624bad40b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Schema in Spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9559c2b-f17d-4d61-aa98-6b74bf73c9ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c2e5af7-9d1f-46c4-b876-8e32ef312fb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "my_schema= StructType([\n",
    "            StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n",
    "            StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n",
    "            StructField(\"count\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1950ed84-c589-4202-9d21-2efca0f139f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3480508081326128>:8\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m flight_df\u001B[38;5;241m=\u001B[39mspark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n",
       "\u001B[1;32m      2\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheader\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfalse\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n",
       "\u001B[1;32m      3\u001B[0m         \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minferSchema\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfalse\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n",
       "\u001B[1;32m      4\u001B[0m           \u001B[38;5;241m.\u001B[39mschema(my_schema)\\\n",
       "\u001B[1;32m      5\u001B[0m             \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmode\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFAILFAST\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n",
       "\u001B[1;32m      6\u001B[0m                 \u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/FileStore/tables/2010_summary.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m----> 8\u001B[0m flight_df\u001B[38;5;241m.\u001B[39mshow(\u001B[38;5;241m5\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:920\u001B[0m, in \u001B[0;36mDataFrame.show\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n",
       "\u001B[1;32m    914\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n",
       "\u001B[1;32m    915\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_A_BOOLEAN\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    916\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvertical\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(vertical)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n",
       "\u001B[1;32m    917\u001B[0m     )\n",
       "\u001B[1;32m    919\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(truncate, \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m truncate:\n",
       "\u001B[0;32m--> 920\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshowString\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvertical\u001B[49m\u001B[43m)\u001B[49m)\n",
       "\u001B[1;32m    921\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    922\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n",
       "\u001B[1;32m    227\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 228\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    230\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n",
       "\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n",
       "\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n",
       "\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n",
       "\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n",
       "\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n",
       "\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
       "\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o487.showString.\n",
       ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (ip-10-172-185-70.us-west-2.compute.internal executor driver): com.databricks.sql.io.FileReadException: Error while reading file dbfs:/FileStore/tables/2010_summary.csv.\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:701)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:670)\n",
       "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:793)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:493)\n",
       "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:483)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
       "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
       "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1741)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n",
       "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1936)\n",
       "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:103)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n",
       "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
       "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:616)\n",
       "\t... 32 more\n",
       "Caused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"count\"\n",
       "Caused by: java.lang.NumberFormatException: For input string: \"count\"\n",
       "\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n",
       "\tat java.lang.Integer.parseInt(Integer.java:580)\n",
       "\tat java.lang.Integer.parseInt(Integer.java:615)\n",
       "\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:304)\n",
       "\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:304)\n",
       "\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6(UnivocityParser.scala:202)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6$adapted(UnivocityParser.scala:202)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:305)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$5(UnivocityParser.scala:202)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.convert(UnivocityParser.scala:360)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:321)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:499)\n",
       "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:92)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n",
       "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
       "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:616)\n",
       "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:793)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:493)\n",
       "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:483)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
       "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
       "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1741)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "\n",
       "Driver stacktrace:\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3440)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3362)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3351)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
       "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3351)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1460)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1460)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1460)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3651)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3589)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3577)\n",
       "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1209)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1197)\n",
       "\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2758)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$runSparkJobs$1(Collector.scala:349)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:293)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:377)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:128)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:135)\n",
       "\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:122)\n",
       "\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:110)\n",
       "\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:92)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$computeResult$1(ResultCacheManager.scala:537)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.collectResult$1(ResultCacheManager.scala:529)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.computeResult(ResultCacheManager.scala:549)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:402)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:395)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:289)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollectResult$1(SparkPlan.scala:506)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:503)\n",
       "\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3458)\n",
       "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4382)\n",
       "\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3168)\n",
       "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$3(Dataset.scala:4373)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:841)\n",
       "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4371)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$9(SQLExecution.scala:258)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:448)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:203)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1073)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:131)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:398)\n",
       "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4371)\n",
       "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3168)\n",
       "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3389)\n",
       "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:315)\n",
       "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:354)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: com.databricks.sql.io.FileReadException: Error while reading file dbfs:/FileStore/tables/2010_summary.csv.\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:701)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:670)\n",
       "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:793)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:493)\n",
       "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:483)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
       "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
       "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1741)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\t... 1 more\n",
       "Caused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n",
       "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1936)\n",
       "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:103)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n",
       "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
       "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:616)\n",
       "\t... 32 more\n",
       "Caused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"count\"\n",
       "Caused by: java.lang.NumberFormatException: For input string: \"count\"\n",
       "\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n",
       "\tat java.lang.Integer.parseInt(Integer.java:580)\n",
       "\tat java.lang.Integer.parseInt(Integer.java:615)\n",
       "\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:304)\n",
       "\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:304)\n",
       "\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6(UnivocityParser.scala:202)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6$adapted(UnivocityParser.scala:202)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:305)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$5(UnivocityParser.scala:202)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.convert(UnivocityParser.scala:360)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:321)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:499)\n",
       "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:92)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n",
       "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
       "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:616)\n",
       "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:793)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:493)\n",
       "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:483)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
       "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
       "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1741)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\nFile \u001B[0;32m<command-3480508081326128>:8\u001B[0m\n\u001B[1;32m      1\u001B[0m flight_df\u001B[38;5;241m=\u001B[39mspark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheader\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfalse\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n\u001B[1;32m      3\u001B[0m         \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minferSchema\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfalse\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n\u001B[1;32m      4\u001B[0m           \u001B[38;5;241m.\u001B[39mschema(my_schema)\\\n\u001B[1;32m      5\u001B[0m             \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmode\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFAILFAST\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n\u001B[1;32m      6\u001B[0m                 \u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/FileStore/tables/2010_summary.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 8\u001B[0m flight_df\u001B[38;5;241m.\u001B[39mshow(\u001B[38;5;241m5\u001B[39m)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:920\u001B[0m, in \u001B[0;36mDataFrame.show\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n\u001B[1;32m    914\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[1;32m    915\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_A_BOOLEAN\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    916\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvertical\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(vertical)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n\u001B[1;32m    917\u001B[0m     )\n\u001B[1;32m    919\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(truncate, \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m truncate:\n\u001B[0;32m--> 920\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshowString\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvertical\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    921\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    922\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    227\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 228\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    230\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o487.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (ip-10-172-185-70.us-west-2.compute.internal executor driver): com.databricks.sql.io.FileReadException: Error while reading file dbfs:/FileStore/tables/2010_summary.csv.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:701)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:670)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:793)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:493)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:483)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1741)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1936)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:103)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:616)\n\t... 32 more\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"count\"\nCaused by: java.lang.NumberFormatException: For input string: \"count\"\n\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n\tat java.lang.Integer.parseInt(Integer.java:580)\n\tat java.lang.Integer.parseInt(Integer.java:615)\n\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:304)\n\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:304)\n\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6(UnivocityParser.scala:202)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6$adapted(UnivocityParser.scala:202)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:305)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$5(UnivocityParser.scala:202)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.convert(UnivocityParser.scala:360)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:321)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:499)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:92)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:616)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:793)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:493)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:483)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1741)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3440)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3362)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3351)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3351)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1460)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1460)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1460)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3651)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3589)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3577)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1209)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1197)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2758)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$runSparkJobs$1(Collector.scala:349)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:293)\n\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:377)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:128)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:135)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:122)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:110)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:92)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$computeResult$1(ResultCacheManager.scala:537)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.collectResult$1(ResultCacheManager.scala:529)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.computeResult(ResultCacheManager.scala:549)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:402)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:395)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:289)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollectResult$1(SparkPlan.scala:506)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:503)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3458)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4382)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3168)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$3(Dataset.scala:4373)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:841)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4371)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$9(SQLExecution.scala:258)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:448)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:203)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1073)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:131)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:398)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4371)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3168)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:315)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:354)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: com.databricks.sql.io.FileReadException: Error while reading file dbfs:/FileStore/tables/2010_summary.csv.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:701)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:670)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:793)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:493)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:483)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1741)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1936)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:103)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:616)\n\t... 32 more\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"count\"\nCaused by: java.lang.NumberFormatException: For input string: \"count\"\n\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n\tat java.lang.Integer.parseInt(Integer.java:580)\n\tat java.lang.Integer.parseInt(Integer.java:615)\n\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:304)\n\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:304)\n\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6(UnivocityParser.scala:202)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6$adapted(UnivocityParser.scala:202)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:305)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$5(UnivocityParser.scala:202)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.convert(UnivocityParser.scala:360)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:321)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:499)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:92)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:616)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:793)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:493)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:483)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1741)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n",
       "errorSummary": "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (ip-10-172-185-70.us-west-2.compute.internal executor driver): com.databricks.sql.io.FileReadException: Error while reading file dbfs:/FileStore/tables/2010_summary.csv.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "flight_df=spark.read.format(\"csv\")\\\n",
    "    .option(\"header\",\"false\")\\\n",
    "        .option(\"inferSchema\",\"false\")\\\n",
    "          .schema(my_schema)\\\n",
    "            .option(\"mode\",\"FAILFAST\")\\\n",
    "                .load(\"/FileStore/tables/2010_summary.csv\")\n",
    "\n",
    "flight_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7648f191-ff37-4778-b7e4-5d8c1f1e5bb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| null|\n|    United States|            Romania|    1|\n|    United States|            Ireland|  264|\n|    United States|              India|   69|\n|            Egypt|      United States|   24|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "flight_df=spark.read.format(\"csv\")\\\n",
    "    .option(\"header\",\"false\")\\\n",
    "        .option(\"inferSchema\",\"false\")\\\n",
    "          .schema(my_schema)\\\n",
    "            .option(\"mode\",\"PERMISSIVE\")\\\n",
    "                .load(\"/FileStore/tables/2010_summary.csv\")\n",
    "\n",
    "flight_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb2bfe64-5823-45f6-a16e-e60412830b59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|    1|\n|    United States|            Ireland|  264|\n|    United States|              India|   69|\n|            Egypt|      United States|   24|\n|Equatorial Guinea|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "flight_df=spark.read.format(\"csv\")\\\n",
    "    .option(\"header\",\"false\")\\\n",
    "        .option(\"skipRows\",1)\\\n",
    "        .option(\"inferSchema\",\"false\")\\\n",
    "          .schema(my_schema)\\\n",
    "            .option(\"mode\",\"PERMISSIVE\")\\\n",
    "                .load(\"/FileStore/tables/2010_summary.csv\")\n",
    "\n",
    "flight_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba0c7c51-c957-4d92-8230-ee2b4d610fab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Handling Corrupted Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d3d97e1-e585-4984-988d-ca8dd5e1645b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " /FileStore/tables/employee.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d926c7f-c081-4886-8c40-8096e688cbc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+\n| id|    name|age|salary|     address| nominee|\n+---+--------+---+------+------------+--------+\n|  1|  Manish| 26| 75000|       bihar|nominee1|\n|  2|  Nikita| 23|100000|uttarpradesh|nominee2|\n|  3|  Pritam| 22|150000|   Bangalore|   India|\n|  4|Prantosh| 17|200000|     Kolkata|   India|\n|  5|  Vikash| 31|300000|        null|nominee5|\n+---+--------+---+------+------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "employee_df=spark.read.format(\"csv\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "        .option(\"inferSchema\",\"true\")\\\n",
    "            .option(\"mode\",\"PERMISSIVE\")\\\n",
    "                .load(\"/FileStore/tables/employee.csv\")\n",
    "\n",
    "employee_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ec399a4-0bc7-4ec0-a991-6655280e7fb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------+------------+--------+\n| id|  name|age|salary|     address| nominee|\n+---+------+---+------+------------+--------+\n|  1|Manish| 26| 75000|       bihar|nominee1|\n|  2|Nikita| 23|100000|uttarpradesh|nominee2|\n|  5|Vikash| 31|300000|        null|nominee5|\n+---+------+---+------+------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "employee_df=spark.read.format(\"csv\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "        .option(\"inferSchema\",\"true\")\\\n",
    "            .option(\"mode\",\"DROPMALFORMED\")\\\n",
    "                .load(\"/FileStore/tables/employee.csv\")\n",
    "\n",
    "employee_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1b3cee2-9219-4371-876d-29dfbe6cf58a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5461b0a-4abb-4b1c-a613-f77a418c57cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp_schema= StructType([\n",
    "            StructField(\"id\", IntegerType(), True),\n",
    "            StructField(\"name\", StringType(), True),\n",
    "            StructField(\"age\", IntegerType(), True),\n",
    "            StructField(\"salary\", IntegerType(), True),\n",
    "            StructField(\"address\", StringType(), True),\n",
    "            StructField(\"nominee\", StringType(), True),\n",
    "            StructField(\"_corrupt_record\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f32a994b-19bb-4197-8b1f-1fc5d94f4bb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+-------------------------------------------+\n|id |name    |age|salary|address     |nominee |_corrupt_record                            |\n+---+--------+---+------+------------+--------+-------------------------------------------+\n|1  |Manish  |26 |75000 |bihar       |nominee1|null                                       |\n|2  |Nikita  |23 |100000|uttarpradesh|nominee2|null                                       |\n|3  |Pritam  |22 |150000|Bangalore   |India   |3,Pritam,22,150000,Bangalore,India,nominee3|\n|4  |Prantosh|17 |200000|Kolkata     |India   |4,Prantosh,17,200000,Kolkata,India,nominee4|\n|5  |Vikash  |31 |300000|null        |nominee5|null                                       |\n+---+--------+---+------+------------+--------+-------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "employee_df=spark.read.format(\"csv\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "        .option(\"inferSchema\",\"true\")\\\n",
    "            .option(\"mode\",\"PERMISSIVE\")\\\n",
    "                .schema(emp_schema)\\\n",
    "                .load(\"/FileStore/tables/employee.csv\")\n",
    "\n",
    "employee_df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "319e1283-9942-44a4-a01e-bff3afdf5fb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/tables/2010_summary.csv</td><td>2010_summary.csv</td><td>7121</td><td>1734530986000</td></tr><tr><td>dbfs:/FileStore/tables/StudentData.csv</td><td>StudentData.csv</td><td>75701</td><td>1656587787000</td></tr><tr><td>dbfs:/FileStore/tables/distinct.txt</td><td>distinct.txt</td><td>900</td><td>1656580239000</td></tr><tr><td>dbfs:/FileStore/tables/employee.csv</td><td>employee.csv</td><td>230</td><td>1734622538000</td></tr><tr><td>dbfs:/FileStore/tables/filter.txt</td><td>filter.txt</td><td>45</td><td>1656579325000</td></tr><tr><td>dbfs:/FileStore/tables/groupreduce.txt</td><td>groupreduce.txt</td><td>3226646</td><td>1656582669000</td></tr><tr><td>dbfs:/FileStore/tables/numbers.txt</td><td>numbers.txt</td><td>29</td><td>1656409579000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/tables/2010_summary.csv",
         "2010_summary.csv",
         7121,
         1734530986000
        ],
        [
         "dbfs:/FileStore/tables/StudentData.csv",
         "StudentData.csv",
         75701,
         1656587787000
        ],
        [
         "dbfs:/FileStore/tables/distinct.txt",
         "distinct.txt",
         900,
         1656580239000
        ],
        [
         "dbfs:/FileStore/tables/employee.csv",
         "employee.csv",
         230,
         1734622538000
        ],
        [
         "dbfs:/FileStore/tables/filter.txt",
         "filter.txt",
         45,
         1656579325000
        ],
        [
         "dbfs:/FileStore/tables/groupreduce.txt",
         "groupreduce.txt",
         3226646,
         1656582669000
        ],
        [
         "dbfs:/FileStore/tables/numbers.txt",
         "numbers.txt",
         29,
         1656409579000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs\n",
    "ls /FileStore/tables/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5302721c-34ac-43d1-80be-ced16ce8e95a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------+------------+--------+---------------+\n|id |name  |age|salary|address     |nominee |_corrupt_record|\n+---+------+---+------+------------+--------+---------------+\n|1  |Manish|26 |75000 |bihar       |nominee1|null           |\n|2  |Nikita|23 |100000|uttarpradesh|nominee2|null           |\n|5  |Vikash|31 |300000|null        |nominee5|null           |\n+---+------+---+------+------------+--------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "employee_df=spark.read.format(\"csv\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "        .option(\"inferSchema\",\"true\")\\\n",
    "                .schema(emp_schema)\\\n",
    "                    .option(\"badRecordsPath\",\"/FileStore/tables/bad_records\")\\\n",
    "                .load(\"/FileStore/tables/employee.csv\")\n",
    "\n",
    "employee_df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18cf9ef5-c952-4a8e-89cb-aa9c00d71111",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/tables/2010_summary.csv</td><td>2010_summary.csv</td><td>7121</td><td>1734530986000</td></tr><tr><td>dbfs:/FileStore/tables/StudentData.csv</td><td>StudentData.csv</td><td>75701</td><td>1656587787000</td></tr><tr><td>dbfs:/FileStore/tables/bad_records/</td><td>bad_records/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/distinct.txt</td><td>distinct.txt</td><td>900</td><td>1656580239000</td></tr><tr><td>dbfs:/FileStore/tables/employee.csv</td><td>employee.csv</td><td>230</td><td>1734622538000</td></tr><tr><td>dbfs:/FileStore/tables/filter.txt</td><td>filter.txt</td><td>45</td><td>1656579325000</td></tr><tr><td>dbfs:/FileStore/tables/groupreduce.txt</td><td>groupreduce.txt</td><td>3226646</td><td>1656582669000</td></tr><tr><td>dbfs:/FileStore/tables/numbers.txt</td><td>numbers.txt</td><td>29</td><td>1656409579000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/tables/2010_summary.csv",
         "2010_summary.csv",
         7121,
         1734530986000
        ],
        [
         "dbfs:/FileStore/tables/StudentData.csv",
         "StudentData.csv",
         75701,
         1656587787000
        ],
        [
         "dbfs:/FileStore/tables/bad_records/",
         "bad_records/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/distinct.txt",
         "distinct.txt",
         900,
         1656580239000
        ],
        [
         "dbfs:/FileStore/tables/employee.csv",
         "employee.csv",
         230,
         1734622538000
        ],
        [
         "dbfs:/FileStore/tables/filter.txt",
         "filter.txt",
         45,
         1656579325000
        ],
        [
         "dbfs:/FileStore/tables/groupreduce.txt",
         "groupreduce.txt",
         3226646,
         1656582669000
        ],
        [
         "dbfs:/FileStore/tables/numbers.txt",
         "numbers.txt",
         29,
         1656409579000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs\n",
    "ls /FileStore/tables/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4a3799a-9fef-4892-bd37-7f0e941f1a14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/tables/bad_records/20241219T155908/bad_records/part-00000-997be771-6ddf-46ec-b3b6-364f603eec20</td><td>part-00000-997be771-6ddf-46ec-b3b6-364f603eec20</td><td>484</td><td>1734623949000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/tables/bad_records/20241219T155908/bad_records/part-00000-997be771-6ddf-46ec-b3b6-364f603eec20",
         "part-00000-997be771-6ddf-46ec-b3b6-364f603eec20",
         484,
         1734623949000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs\n",
    "ls /FileStore/tables/bad_records/20241219T155908/bad_records/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db52077e-26bf-48ed-a026-232db2058ee7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+--------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------+\n|path                               |reason                                                                                                                          |record                                     |\n+-----------------------------------+--------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------+\n|dbfs:/FileStore/tables/employee.csv|org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 3,Pritam,22,150000,Bangalore,India,nominee3|3,Pritam,22,150000,Bangalore,India,nominee3|\n|dbfs:/FileStore/tables/employee.csv|org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 4,Prantosh,17,200000,Kolkata,India,nominee4|4,Prantosh,17,200000,Kolkata,India,nominee4|\n+-----------------------------------+--------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "bad_data_df= spark.read.format('json')\\\n",
    "    .load('/FileStore/tables/bad_records/20241219T155908/bad_records/')\n",
    "\n",
    "bad_data_df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d476560-546a-4545-b122-b2b0e68fc8a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Read JSON File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e0ced7c-ca87-422a-884c-b8d66d7397a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "File uploaded to /FileStore/tables/corrupted_json.json\n",
    "File uploaded to /FileStore/tables/line_delimited_json.json\n",
    "File uploaded to /FileStore/tables/Multi_line_correct.json\n",
    "File uploaded to /FileStore/tables/Multi_line_incorrect.json\n",
    "File uploaded to /FileStore/tables/single_file_json_with_extra_fields.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "315ebe3b-0836-4e19-8430-306096e1a28e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+\n|age|    name|salary|\n+---+--------+------+\n| 20|  Manish| 20000|\n| 25|  Nikita| 21000|\n| 16|  Pritam| 22000|\n| 35|Prantosh| 25000|\n| 67|  Vikash| 40000|\n+---+--------+------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.read.format('json')\\\n",
    "    .option('inferschema','true')\\\n",
    "        .option('mode','PERMISSIVE')\\\n",
    "            .load('/FileStore/tables/line_delimited_json.json').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "820940ea-da54-4a8c-ba17-9599dc1ef092",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=2503095305578706#setting/sparkui/1219-170843-phy8shn4/driver-355865280355387655\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=2503095305578706#setting/sparkui/1219-170843-phy8shn4/driver-355865280355387655\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0874bbb5-3908-4630-a72d-da12534f05a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------+------+\n|age|gender|    name|salary|\n+---+------+--------+------+\n| 20|  null|  Manish| 20000|\n| 25|  null|  Nikita| 21000|\n| 16|  null|  Pritam| 22000|\n| 35|  null|Prantosh| 25000|\n| 67|     M|  Vikash| 40000|\n+---+------+--------+------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.read.format('json')\\\n",
    "    .option('inferschema','true')\\\n",
    "        .option('mode','PERMISSIVE')\\\n",
    "            .load('/FileStore/tables/single_file_json_with_extra_fields.json').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dc3d24d-06e7-4dbf-a0e6-d04a61b6fbff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+\n|age|    name|salary|\n+---+--------+------+\n| 20|  Manish| 20000|\n| 25|  Nikita| 21000|\n| 16|  Pritam| 22000|\n| 35|Prantosh| 25000|\n| 67|  Vikash| 40000|\n+---+--------+------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.read.format('json')\\\n",
    "    .option('inferschema','true')\\\n",
    "        .option('mode','PERMISSIVE')\\\n",
    "            .option('multiline','true')\\\n",
    "            .load('/FileStore/tables/Multi_line_correct.json').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73e5a77c-72c1-4a87-b4de-f5d7264a226b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n|age|  name|salary|\n+---+------+------+\n| 20|Manish| 20000|\n+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.read.format('json')\\\n",
    "    .option('inferschema','true')\\\n",
    "        .option('mode','PERMISSIVE')\\\n",
    "            .option('multiline','true')\\\n",
    "            .load('/FileStore/tables/Multi_line_incorrect.json').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b88c04e1-294d-4eca-bd62-ceba5081ca3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----+--------+------+\n|_corrupt_record                         |age |name    |salary|\n+----------------------------------------+----+--------+------+\n|null                                    |20  |Manish  |20000 |\n|null                                    |25  |Nikita  |21000 |\n|null                                    |16  |Pritam  |22000 |\n|null                                    |35  |Prantosh|25000 |\n|{\"name\":\"Vikash\",\"age\":67,\"salary\":40000|null|null    |null  |\n+----------------------------------------+----+--------+------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.read.format('json')\\\n",
    "    .option('inferschema','true')\\\n",
    "        .option('mode','PERMISSIVE')\\\n",
    "            .load('/FileStore/tables/corrupted_json.json').show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "354c8902-99df-4a19-a8cc-9d509c74cd32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2292ef6e-171c-433f-b7b7-c67d015bc628",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "File uploaded to /FileStore/tables/part_r_00000_1a9822ba_b8fb_4d8e_844a_ea30d0801b9e_gz.parquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6fce293-5d97-4503-b5b3-e154cef4891c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+--------------------+-------------------+-----+\n|       United States|            Romania|    1|\n|       United States|            Ireland|  264|\n|       United States|              India|   69|\n|               Egypt|      United States|   24|\n|   Equatorial Guinea|      United States|    1|\n|       United States|          Singapore|   25|\n|       United States|            Grenada|   54|\n|          Costa Rica|      United States|  477|\n|             Senegal|      United States|   29|\n|       United States|   Marshall Islands|   44|\n|              Guyana|      United States|   17|\n|       United States|       Sint Maarten|   53|\n|               Malta|      United States|    1|\n|             Bolivia|      United States|   46|\n|            Anguilla|      United States|   21|\n|Turks and Caicos ...|      United States|  136|\n|       United States|        Afghanistan|    2|\n|Saint Vincent and...|      United States|    1|\n|               Italy|      United States|  390|\n|       United States|             Russia|  156|\n+--------------------+-------------------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet('/FileStore/tables/part_r_00000_1a9822ba_b8fb_4d8e_844a_ea30d0801b9e_gz.parquet').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c73fa422-a26c-4a19-9f68-eb8f883f535e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Write data  /FileStore/tables/person.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "512bae9a-9b3f-4fbb-8380-de5f96638099",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=spark.read.format('csv')\\\n",
    "  .option('header','true')\\\n",
    "    .option('inferschema','true')\\\n",
    "      .load('/FileStore/tables/employees.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d1dfb56-f9cf-43d1-bbdd-997dac58ab31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------+--------+----------+-----------+\n| id|      name|     age|  salary|   address|     gender|\n+---+----------+--------+--------+----------+-----------+\n|  1|    Manish|    26.0| 75000.0|     INDIA|          m|\n|  2|    Nikita|    23.0|100000.0|       USA|          f|\n|  3|    Pritam|    22.0|150000.0|     INDIA|          m|\n|  4|  Prantosh|    17.0|200000.0|     JAPAN|          m|\n|  5|    Vikash|    31.0|300000.0|       USA|          m|\n|  6|     Rahul|    55.0|300000.0|     INDIA|          m|\n|  7|      Raju|    67.0|540000.0|       USA|          m|\n|  8|   Praveen|    28.0| 70000.0|     JAPAN|          m|\n|  9|       Dev|    32.0|150000.0|     JAPAN|          m|\n| 10|    Sherin|    16.0| 25000.0|    RUSSIA|          f|\n| 11|      Ragu|    12.0| 35000.0|     INDIA|          f|\n| 12|     Sweta|    43.0|200000.0|     INDIA|          f|\n| 13|   Raushan|    48.0|650000.0|       USA|          m|\n| 14|    Mukesh|    36.0| 95000.0|    RUSSIA|          m|\n| 15|   Prakash|    52.0|750000.0|     INDIA|          m|\n+---+----------+--------+--------+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d34b5bf-5b82-4052-bdac-f04c895febd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: integer (nullable = true)\n |--  name: string (nullable = true)\n |--      age: double (nullable = true)\n |--   salary: double (nullable = true)\n |--    address: string (nullable = true)\n |--     gender: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9d55b13-76b0-4f1d-bf6d-210276fb7c02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format('csv')\\\n",
    "  .option('header','true')\\\n",
    "    .option('mode','overwrite')\\\n",
    "      .option('path','/FileStore/tables/csv_write/')\\\n",
    "          .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12e16ba8-4fc0-42ec-9570-f04fdd860837",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[4]: [FileInfo(path='dbfs:/FileStore/tables/csv_write/_SUCCESS', name='_SUCCESS', size=0, modificationTime=1734881411000),\n FileInfo(path='dbfs:/FileStore/tables/csv_write/_committed_1530211572815462398', name='_committed_1530211572815462398', size=111, modificationTime=1734881411000),\n FileInfo(path='dbfs:/FileStore/tables/csv_write/_started_1530211572815462398', name='_started_1530211572815462398', size=0, modificationTime=1734881410000),\n FileInfo(path='dbfs:/FileStore/tables/csv_write/part-00000-tid-1530211572815462398-7483867c-4f7d-4d77-8a78-6abee8dca82e-3-1-c000.csv', name='part-00000-tid-1530211572815462398-7483867c-4f7d-4d77-8a78-6abee8dca82e-3-1-c000.csv', size=490, modificationTime=1734881410000)]"
     ]
    }
   ],
   "source": [
    "dbutils.fs.ls('/FileStore/tables/csv_write/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fffe68df-26f2-427f-b718-b975795046b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.repartition(3).write.format('csv')\\\n",
    "  .option('header','true')\\\n",
    "    .option('mode','overwrite')\\\n",
    "      .option('path','/FileStore/tables/csv_write_repartition/')\\\n",
    "          .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccdb7553-ff61-4e6e-9ea6-9db1088f871a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[6]: [FileInfo(path='dbfs:/FileStore/tables/csv_write_repartition/_SUCCESS', name='_SUCCESS', size=0, modificationTime=1734881616000),\n FileInfo(path='dbfs:/FileStore/tables/csv_write_repartition/_committed_5507568321707987609', name='_committed_5507568321707987609', size=285, modificationTime=1734881616000),\n FileInfo(path='dbfs:/FileStore/tables/csv_write_repartition/_started_5507568321707987609', name='_started_5507568321707987609', size=0, modificationTime=1734881615000),\n FileInfo(path='dbfs:/FileStore/tables/csv_write_repartition/part-00000-tid-5507568321707987609-dbbbb398-b5b1-46f2-948c-28a6bb88ad67-5-1-c000.csv', name='part-00000-tid-5507568321707987609-dbbbb398-b5b1-46f2-948c-28a6bb88ad67-5-1-c000.csv', size=184, modificationTime=1734881616000),\n FileInfo(path='dbfs:/FileStore/tables/csv_write_repartition/part-00001-tid-5507568321707987609-dbbbb398-b5b1-46f2-948c-28a6bb88ad67-6-1-c000.csv', name='part-00001-tid-5507568321707987609-dbbbb398-b5b1-46f2-948c-28a6bb88ad67-6-1-c000.csv', size=184, modificationTime=1734881616000),\n FileInfo(path='dbfs:/FileStore/tables/csv_write_repartition/part-00002-tid-5507568321707987609-dbbbb398-b5b1-46f2-948c-28a6bb88ad67-7-1-c000.csv', name='part-00002-tid-5507568321707987609-dbbbb398-b5b1-46f2-948c-28a6bb88ad67-7-1-c000.csv', size=190, modificationTime=1734881616000)]"
     ]
    }
   ],
   "source": [
    "dbutils.fs.ls('/FileStore/tables/csv_write_repartition/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b330f7a9-34c4-43a6-a6a2-fe1c281f3113",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Partitioning and Bucketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58a2cbde-a114-4303-bdd3-731644ad18d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- age: integer (nullable = true)\n |-- salary: integer (nullable = true)\n |-- address: string (nullable = true)\n |-- gender: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acf38063-ec82-4ab4-8c5f-aa94b8a8ca12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format('csv')\\\n",
    "  .option('header','true')\\\n",
    "    .option('mode','overwrite')\\\n",
    "      .option('path','/FileStore/tables/partitionBy_address/')\\\n",
    "       .partitionBy('address')\\\n",
    "          .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2af1a25a-42c3-4131-88ab-fb456dbed17c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[15]: [FileInfo(path='dbfs:/FileStore/tables/partitionBy_address/_SUCCESS', name='_SUCCESS', size=0, modificationTime=1734884098000),\n FileInfo(path='dbfs:/FileStore/tables/partitionBy_address/address=INDIA/', name='address=INDIA/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partitionBy_address/address=JAPAN/', name='address=JAPAN/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partitionBy_address/address=RUSSIA/', name='address=RUSSIA/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partitionBy_address/address=USA/', name='address=USA/', size=0, modificationTime=0)]"
     ]
    }
   ],
   "source": [
    "dbutils.fs.ls('/FileStore/tables/partitionBy_address/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae0d52f8-62ce-41bb-98e3-e68b47f43663",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format('csv')\\\n",
    "  .option('header','true')\\\n",
    "    .option('mode','overwrite')\\\n",
    "      .option('path','/FileStore/tables/partitionBy_address_gender/')\\\n",
    "       .partitionBy('address','gender')\\\n",
    "          .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0db1040-87a2-49b6-bd85-c276521dbd3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[17]: [FileInfo(path='dbfs:/FileStore/tables/partitionBy_address_gender/_SUCCESS', name='_SUCCESS', size=0, modificationTime=1734884734000),\n FileInfo(path='dbfs:/FileStore/tables/partitionBy_address_gender/address=INDIA/', name='address=INDIA/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partitionBy_address_gender/address=JAPAN/', name='address=JAPAN/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partitionBy_address_gender/address=RUSSIA/', name='address=RUSSIA/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partitionBy_address_gender/address=USA/', name='address=USA/', size=0, modificationTime=0)]"
     ]
    }
   ],
   "source": [
    "dbutils.fs.ls('/FileStore/tables/partitionBy_address_gender/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a4d97e2-89a5-4fb4-b60c-eef25d4d6f3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[18]: [FileInfo(path='dbfs:/FileStore/tables/partitionBy_address_gender/address=INDIA/gender=f/', name='gender=f/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partitionBy_address_gender/address=INDIA/gender=m/', name='gender=m/', size=0, modificationTime=0)]"
     ]
    }
   ],
   "source": [
    "dbutils.fs.ls('dbfs:/FileStore/tables/partitionBy_address_gender/address=INDIA/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e3ca027-9120-43f8-8664-8ba6089946fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format('csv')\\\n",
    "  .option('header','true')\\\n",
    "    .option('mode','overwrite')\\\n",
    "      .option('path','/FileStore/tables/bucket_by_id/')\\\n",
    "       .bucketBy(3,'id')\\\n",
    "          .saveAsTable('bucket_by_id_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1bdcaed-9c76-4caa-be33-e55b7925a583",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[22]: [FileInfo(path='dbfs:/FileStore/tables/bucket_by_id/_SUCCESS', name='_SUCCESS', size=0, modificationTime=1734885355000),\n FileInfo(path='dbfs:/FileStore/tables/bucket_by_id/_committed_6871011973106432807', name='_committed_6871011973106432807', size=306, modificationTime=1734885355000),\n FileInfo(path='dbfs:/FileStore/tables/bucket_by_id/_started_6871011973106432807', name='_started_6871011973106432807', size=0, modificationTime=1734885354000),\n FileInfo(path='dbfs:/FileStore/tables/bucket_by_id/part-00000-tid-6871011973106432807-a2caea16-0069-4cef-a54c-9f7e7364d24f-13-1_00000.c000.csv', name='part-00000-tid-6871011973106432807-a2caea16-0069-4cef-a54c-9f7e7364d24f-13-1_00000.c000.csv', size=270, modificationTime=1734885354000),\n FileInfo(path='dbfs:/FileStore/tables/bucket_by_id/part-00000-tid-6871011973106432807-a2caea16-0069-4cef-a54c-9f7e7364d24f-13-2_00001.c000.csv', name='part-00000-tid-6871011973106432807-a2caea16-0069-4cef-a54c-9f7e7364d24f-13-2_00001.c000.csv', size=113, modificationTime=1734885355000),\n FileInfo(path='dbfs:/FileStore/tables/bucket_by_id/part-00000-tid-6871011973106432807-a2caea16-0069-4cef-a54c-9f7e7364d24f-13-3_00002.c000.csv', name='part-00000-tid-6871011973106432807-a2caea16-0069-4cef-a54c-9f7e7364d24f-13-3_00002.c000.csv', size=115, modificationTime=1734885355000)]"
     ]
    }
   ],
   "source": [
    "dbutils.fs.ls('/FileStore/tables/bucket_by_id/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f13372bc-6e30-45f7-b09c-4b85e5f6eb4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bd7f7e2-4ee0-4a3a-9c95-c854230fa0cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "my_data = [(1,1),(2,1),(3,1),(4,2),(5,1),(6,2),(7,2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6045cc4-8163-4308-9eeb-a9fb8e50b96e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "my_schema = ['id','num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99cdfca3-d8f6-4ee8-9c03-18998a779982",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "my_df=spark.createDataFrame(data=my_data,schema=my_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cefa492b-5a26-4550-a7de-80cde6025a5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n| id|num|\n+---+---+\n|  1|  1|\n|  2|  1|\n|  3|  1|\n|  4|  2|\n|  5|  1|\n|  6|  2|\n|  7|  2|\n+---+---+\n\n"
     ]
    }
   ],
   "source": [
    "my_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd07286b-73aa-496f-9ff7-0b37c8a54239",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b8785d9-c944-4586-a381-5192a0bff362",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+\n| id|    name|age|salary|     address| nominee|\n+---+--------+---+------+------------+--------+\n|  1|  Manish| 26| 75000|       bihar|nominee1|\n|  2|  Nikita| 23|100000|uttarpradesh|nominee2|\n|  3|  Pritam| 22|150000|   Bangalore|   India|\n|  4|Prantosh| 17|200000|     Kolkata|   India|\n|  5|  Vikash| 31|300000|        null|nominee5|\n+---+--------+---+------+------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "employee_df=spark.read.format(\"csv\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "        .option(\"inferSchema\",\"true\")\\\n",
    "            .option(\"mode\",\"PERMISSIVE\")\\\n",
    "                .load(\"/FileStore/tables/employee.csv\")\n",
    "\n",
    "employee_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4619f0be-925f-49c8-bbc2-083d9001a7e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|    name|\n+--------+\n|  Manish|\n|  Nikita|\n|  Pritam|\n|Prantosh|\n|  Vikash|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "employee_df.select('name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8efe5a0-1168-4628-890d-fa869eca0cd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|    name|\n+--------+\n|  Manish|\n|  Nikita|\n|  Pritam|\n|Prantosh|\n|  Vikash|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "employee_df.select(col('name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4450c797-dd0e-4761-a029-44fdc837ae9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-849915643878402>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43memployee_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mid + 5\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3023\u001B[0m, in \u001B[0;36mDataFrame.select\u001B[0;34m(self, *cols)\u001B[0m\n",
       "\u001B[1;32m   2978\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumnOrName\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n",
       "\u001B[1;32m   2979\u001B[0m     \u001B[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001B[39;00m\n",
       "\u001B[1;32m   2980\u001B[0m \n",
       "\u001B[1;32m   2981\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   3021\u001B[0m \u001B[38;5;124;03m    +-----+---+\u001B[39;00m\n",
       "\u001B[1;32m   3022\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 3023\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jcols\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcols\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   3024\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `id + 5` cannot be resolved. Did you mean one of the following? [`id`, `address`, `age`, `name`, `nominee`].;\n",
       "'Project ['id + 5]\n",
       "+- Relation [id#50,name#51,age#52,salary#53,address#54,nominee#55] csv\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-849915643878402>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43memployee_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mid + 5\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3023\u001B[0m, in \u001B[0;36mDataFrame.select\u001B[0;34m(self, *cols)\u001B[0m\n\u001B[1;32m   2978\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumnOrName\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   2979\u001B[0m     \u001B[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001B[39;00m\n\u001B[1;32m   2980\u001B[0m \n\u001B[1;32m   2981\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3021\u001B[0m \u001B[38;5;124;03m    +-----+---+\u001B[39;00m\n\u001B[1;32m   3022\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 3023\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jcols\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcols\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3024\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `id + 5` cannot be resolved. Did you mean one of the following? [`id`, `address`, `age`, `name`, `nominee`].;\n'Project ['id + 5]\n+- Relation [id#50,name#51,age#52,salary#53,address#54,nominee#55] csv\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `id + 5` cannot be resolved. Did you mean one of the following? [`id`, `address`, `age`, `name`, `nominee`].;\n'Project ['id + 5]\n+- Relation [id#50,name#51,age#52,salary#53,address#54,nominee#55] csv\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "employee_df.select('id + 5').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "851c5cc6-ab31-4e5c-9036-ace9e288c98d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|(id + 5)|\n+--------+\n|       6|\n|       7|\n|       8|\n|       9|\n|      10|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "employee_df.select(col('id') + 5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97f2af67-8196-4def-a6cf-6ac794c03486",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+\n| id|    name|age|\n+---+--------+---+\n|  1|  Manish| 26|\n|  2|  Nikita| 23|\n|  3|  Pritam| 22|\n|  4|Prantosh| 17|\n|  5|  Vikash| 31|\n+---+--------+---+\n\n"
     ]
    }
   ],
   "source": [
    "employee_df.select('id','name','age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a66da04-8af2-4983-b49d-01f79fbb92b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+\n| id|    name|age|\n+---+--------+---+\n|  1|  Manish| 26|\n|  2|  Nikita| 23|\n|  3|  Pritam| 22|\n|  4|Prantosh| 17|\n|  5|  Vikash| 31|\n+---+--------+---+\n\n"
     ]
    }
   ],
   "source": [
    "employee_df.select(col('id'),col('name'),col('age')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94cf5005-66d8-4c5f-b171-a69fa2f77770",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+------------+\n| id|    name|salary|     address|\n+---+--------+------+------------+\n|  1|  Manish| 75000|       bihar|\n|  2|  Nikita|100000|uttarpradesh|\n|  3|  Pritam|150000|   Bangalore|\n|  4|Prantosh|200000|     Kolkata|\n|  5|  Vikash|300000|        null|\n+---+--------+------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "employee_df.select('id',col('name'),employee_df['salary'],employee_df.address).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62476466-bec7-45ea-a2ca-f5c7514e1f6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|(id + 5)|\n+--------+\n|       6|\n|       7|\n|       8|\n|       9|\n|      10|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "employee_df.select(expr('id + 5')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ecb32a7-572c-4b02-945f-918306d25494",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------------+\n|emp_id|emp_name|concat(name, address)|\n+------+--------+---------------------+\n|     1|  Manish|          Manishbihar|\n|     2|  Nikita|   Nikitauttarpradesh|\n|     3|  Pritam|      PritamBangalore|\n|     4|Prantosh|      PrantoshKolkata|\n|     5|  Vikash|                 null|\n+------+--------+---------------------+\n\n"
     ]
    }
   ],
   "source": [
    "employee_df.select(expr('id as emp_id'),expr('name as emp_name'),expr('concat(name,address)')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7ecc394-d63a-4d8d-b0bd-fa02e53dea2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----------------------------+\n|emp_id|emp_name|email                       |\n+------+--------+----------------------------+\n|1     |Manish  |Manishbihar@gmail.com       |\n|2     |Nikita  |Nikitauttarpradesh@gmail.com|\n|3     |Pritam  |PritamBangalore@gmail.com   |\n|4     |Prantosh|PrantoshKolkata@gmail.com   |\n|5     |Vikash  |null                        |\n+------+--------+----------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "employee_df.select(expr('id as emp_id'),expr('name as emp_name'),expr('concat(name,address,\"@gmail.com\") as email')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6c8a6f4-0a4f-4157-b142-e8fe74807e45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "employee_df.createOrReplaceTempView('emp_tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36305a8d-d0b0-4c04-9a2c-20c8feac1424",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+\n| id|    name|age|salary|     address| nominee|\n+---+--------+---+------+------------+--------+\n|  1|  Manish| 26| 75000|       bihar|nominee1|\n|  2|  Nikita| 23|100000|uttarpradesh|nominee2|\n|  3|  Pritam| 22|150000|   Bangalore|   India|\n|  4|Prantosh| 17|200000|     Kolkata|   India|\n|  5|  Vikash| 31|300000|        null|nominee5|\n+---+--------+---+------+------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select * from emp_tbl \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0f46eed-3e09-46d6-a03f-c5e84087a529",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+\n| id|    name|age|\n+---+--------+---+\n|  1|  Manish| 26|\n|  2|  Nikita| 23|\n|  3|  Pritam| 22|\n|  4|Prantosh| 17|\n|  5|  Vikash| 31|\n+---+--------+---+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select id,name,age from emp_tbl \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33776c12-9b01-4573-b11d-6e0cdc4fedaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Transformation - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "deaea5fc-fe44-421e-8ce2-f4a505aea0ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+\n| id|    name|age|salary|     address| nominee|\n+---+--------+---+------+------------+--------+\n|  1|  Manish| 26| 75000|       bihar|nominee1|\n|  2|  Nikita| 23|100000|uttarpradesh|nominee2|\n|  3|  Pritam| 22|150000|   Bangalore|   India|\n|  4|Prantosh| 17|200000|     Kolkata|   India|\n|  5|  Vikash| 31|300000|        null|nominee5|\n+---+--------+---+------+------------+--------+\n\nroot\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- age: integer (nullable = true)\n |-- salary: integer (nullable = true)\n |-- address: string (nullable = true)\n |-- nominee: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "employee_df=spark.read.format(\"csv\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "        .option(\"inferSchema\",\"true\")\\\n",
    "            .option(\"mode\",\"PERMISSIVE\")\\\n",
    "                .load(\"/FileStore/tables/employee.csv\")\n",
    "\n",
    "employee_df.show()\n",
    "\n",
    "employee_df.printSchema()\n",
    "\n",
    "employee_df.createOrReplaceTempView('emp_tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a5ee673-eabe-4996-9ed6-14faaa660f4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+\n|emp_id|    name|age|\n+------+--------+---+\n|     1|  Manish| 26|\n|     2|  Nikita| 23|\n|     3|  Pritam| 22|\n|     4|Prantosh| 17|\n|     5|  Vikash| 31|\n+------+--------+---+\n\n"
     ]
    }
   ],
   "source": [
    "# Alias\n",
    "\n",
    "employee_df.select(col('id').alias('emp_id'),'name','age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "100925d0-47f5-4bbf-9d67-c15787698ea3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+-------+--------+\n| id|    name|age|salary|address| nominee|\n+---+--------+---+------+-------+--------+\n|  4|Prantosh| 17|200000|Kolkata|   India|\n|  5|  Vikash| 31|300000|   null|nominee5|\n+---+--------+---+------+-------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# Filter\n",
    "\n",
    "employee_df.filter(col('salary')>150000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da07e65a-b044-43bd-9bcf-faed9e3a5d55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPy4JError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4305431487598996>:3\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Filter\u001B[39;00m\n",
       "\u001B[0;32m----> 3\u001B[0m employee_df\u001B[38;5;241m.\u001B[39mfilter(col(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msalary\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m>\u001B[39m\u001B[38;5;241;43m150000\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m&\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mcol\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mage\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m<\u001B[39m\u001B[38;5;241m18\u001B[39m)\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/column.py:164\u001B[0m, in \u001B[0;36m_bin_op.<locals>._\u001B[0;34m(self, other)\u001B[0m\n",
       "\u001B[1;32m    159\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_\u001B[39m(\n",
       "\u001B[1;32m    160\u001B[0m     \u001B[38;5;28mself\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumn\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    161\u001B[0m     other: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumn\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLiteralType\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDecimalLiteral\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDateTimeLiteral\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n",
       "\u001B[1;32m    162\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumn\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[1;32m    163\u001B[0m     jc \u001B[38;5;241m=\u001B[39m other\u001B[38;5;241m.\u001B[39m_jc \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(other, Column) \u001B[38;5;28;01melse\u001B[39;00m other\n",
       "\u001B[0;32m--> 164\u001B[0m     njc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43mjc\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    165\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Column(njc)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n",
       "\u001B[1;32m    227\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 228\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    230\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:330\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n",
       "\u001B[1;32m    326\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n",
       "\u001B[1;32m    327\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    328\u001B[0m             \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n",
       "\u001B[1;32m    329\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 330\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n",
       "\u001B[1;32m    331\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    332\u001B[0m             \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
       "\u001B[1;32m    333\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    334\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n",
       "\u001B[1;32m    335\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    336\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name))\n",
       "\n",
       "\u001B[0;31mPy4JError\u001B[0m: An error occurred while calling o553.and. Trace:\n",
       "py4j.Py4JException: Method and([class java.lang.Integer]) does not exist\n",
       "\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:341)\n",
       "\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:349)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:297)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-4305431487598996>:3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Filter\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m employee_df\u001B[38;5;241m.\u001B[39mfilter(col(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msalary\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m>\u001B[39m\u001B[38;5;241;43m150000\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m&\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mcol\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mage\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m<\u001B[39m\u001B[38;5;241m18\u001B[39m)\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/column.py:164\u001B[0m, in \u001B[0;36m_bin_op.<locals>._\u001B[0;34m(self, other)\u001B[0m\n\u001B[1;32m    159\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_\u001B[39m(\n\u001B[1;32m    160\u001B[0m     \u001B[38;5;28mself\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumn\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    161\u001B[0m     other: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumn\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLiteralType\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDecimalLiteral\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDateTimeLiteral\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    162\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumn\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    163\u001B[0m     jc \u001B[38;5;241m=\u001B[39m other\u001B[38;5;241m.\u001B[39m_jc \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(other, Column) \u001B[38;5;28;01melse\u001B[39;00m other\n\u001B[0;32m--> 164\u001B[0m     njc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43mjc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    165\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Column(njc)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    227\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 228\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    230\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:330\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    326\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m             \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 330\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m             \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n\u001B[1;32m    333\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    334\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    335\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    336\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name))\n\n\u001B[0;31mPy4JError\u001B[0m: An error occurred while calling o553.and. Trace:\npy4j.Py4JException: Method and([class java.lang.Integer]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:341)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:349)\n\tat py4j.Gateway.invoke(Gateway.java:297)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\n\n",
       "errorSummary": "py4j.Py4JException: Method and([class java.lang.Integer]) does not exist",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Filter\n",
    "\n",
    "employee_df.filter(col('salary')>150000 & col('age')<18).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22c71379-4d7a-4cb4-9583-7c817b6e411e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+-------+-------+\n| id|    name|age|salary|address|nominee|\n+---+--------+---+------+-------+-------+\n|  4|Prantosh| 17|200000|Kolkata|  India|\n+---+--------+---+------+-------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# Filter\n",
    "\n",
    "employee_df.filter((col('salary')>150000) & (col('age')<18)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e3cf9c8-af59-4cc2-a5cc-f78890ecd2ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+-------+--------+\n| id|    name|age|salary|address| nominee|\n+---+--------+---+------+-------+--------+\n|  4|Prantosh| 17|200000|Kolkata|   India|\n|  5|  Vikash| 31|300000|   null|nominee5|\n+---+--------+---+------+-------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# Where\n",
    "employee_df.where(col('salary')>150000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86f5cd1c-eb0d-488d-b1e1-fb2ae63be69e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+---------+\n| id|    name|age|salary|     address| nominee|Last_name|\n+---+--------+---+------+------------+--------+---------+\n|  1|  Manish| 26| 75000|       bihar|nominee1|    Kumar|\n|  2|  Nikita| 23|100000|uttarpradesh|nominee2|    Kumar|\n|  3|  Pritam| 22|150000|   Bangalore|   India|    Kumar|\n|  4|Prantosh| 17|200000|     Kolkata|   India|    Kumar|\n|  5|  Vikash| 31|300000|        null|nominee5|    Kumar|\n+---+--------+---+------+------------+--------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# Literal\n",
    "\n",
    "employee_df.select('*',lit('Kumar').alias('Last_name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53f77468-c6a0-476a-aeb2-8e97cdc37716",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+-------+\n| id|    name|age|salary|     address| nominee|surname|\n+---+--------+---+------+------------+--------+-------+\n|  1|  Manish| 26| 75000|       bihar|nominee1|  Singh|\n|  2|  Nikita| 23|100000|uttarpradesh|nominee2|  Singh|\n|  3|  Pritam| 22|150000|   Bangalore|   India|  Singh|\n|  4|Prantosh| 17|200000|     Kolkata|   India|  Singh|\n|  5|  Vikash| 31|300000|        null|nominee5|  Singh|\n+---+--------+---+------+------------+--------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# Add Column\n",
    "employee_df.withColumn('surname',lit('Singh')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "902111b2-d7d6-4149-a25a-b4cdbeead9c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+------+------------+--------+\n|emp_id|    name|age|salary|     address| nominee|\n+------+--------+---+------+------------+--------+\n|     1|  Manish| 26| 75000|       bihar|nominee1|\n|     2|  Nikita| 23|100000|uttarpradesh|nominee2|\n|     3|  Pritam| 22|150000|   Bangalore|   India|\n|     4|Prantosh| 17|200000|     Kolkata|   India|\n|     5|  Vikash| 31|300000|        null|nominee5|\n+------+--------+---+------+------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# Rename column\n",
    "\n",
    "employee_df.withColumnRenamed('id','emp_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aae1c2c8-9ca7-4c45-97ca-4ee446949482",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- age: integer (nullable = true)\n |-- salary: integer (nullable = true)\n |-- address: string (nullable = true)\n |-- nominee: string (nullable = true)\n\nroot\n |-- id: string (nullable = true)\n |-- name: string (nullable = true)\n |-- age: integer (nullable = true)\n |-- salary: integer (nullable = true)\n |-- address: string (nullable = true)\n |-- nominee: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Casting\n",
    "\n",
    "employee_df.printSchema()\n",
    "\n",
    "employee_df.withColumn('id',col('id').cast('string')).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2c6bae2-9060-41d3-99f0-151c8be99147",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: string (nullable = true)\n |-- name: string (nullable = true)\n |-- age: integer (nullable = true)\n |-- salary: long (nullable = true)\n |-- address: string (nullable = true)\n |-- nominee: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "employee_df.withColumn('id',col('id').cast('string'))\\\n",
    "    .withColumn('salary',col('salary').cast('long'))\\\n",
    "    .printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edc417c5-a9e7-4f6d-82f7-5db48e535c0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------------+--------+\n|age|salary|     address| nominee|\n+---+------+------------+--------+\n| 26| 75000|       bihar|nominee1|\n| 23|100000|uttarpradesh|nominee2|\n| 22|150000|   Bangalore|   India|\n| 17|200000|     Kolkata|   India|\n| 31|300000|        null|nominee5|\n+---+------+------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "employee_df.drop('id',col('name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "081e4668-fca0-43e3-b176-3122655d64d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58fa1840-9c7f-4b7c-9a42-6f24c4997350",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+-------+-------+\n| id|    name|age|salary|address|nominee|\n+---+--------+---+------+-------+-------+\n|  4|Prantosh| 17|200000|Kolkata|  India|\n+---+--------+---+------+-------+-------+\n\n+---+--------+---+------+-------+-------+---------+\n| id|    name|age|salary|address|nominee|last_name|\n+---+--------+---+------+-------+-------+---------+\n|  4|Prantosh| 17|200000|Kolkata|  India|    Kumar|\n+---+--------+---+------+-------+-------+---------+\n\n+---+--------+---+------+-------+-------+---------+--------------+\n| id|    name|age|salary|address|nominee|last_name|     full_name|\n+---+--------+---+------+-------+-------+---------+--------------+\n|  4|Prantosh| 17|200000|Kolkata|  India|    Kumar|Prantosh Kumar|\n+---+--------+---+------+-------+-------+---------+--------------+\n\n+---+--------+---+------+-------+-------+---------+--------------+------+\n| id|    name|age|salary|address|nominee|last_name|     full_name|emp_id|\n+---+--------+---+------+-------+-------+---------+--------------+------+\n|  4|Prantosh| 17|200000|Kolkata|  India|    Kumar|Prantosh Kumar|     4|\n+---+--------+---+------+-------+-------+---------+--------------+------+\n\nroot\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- age: integer (nullable = true)\n |-- salary: integer (nullable = true)\n |-- address: string (nullable = true)\n |-- nominee: string (nullable = true)\n |-- last_name: string (nullable = false)\n |-- full_name: string (nullable = true)\n |-- id: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select * from emp_tbl where salary>150000 and age<18 \"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\" select *,'Kumar' as last_name from emp_tbl where salary>150000 and age<18 \"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\" select *,'Kumar' as last_name , concat(name,' ',last_name) as full_name from emp_tbl where salary>150000 and age<18 \"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\" select *,'Kumar' as last_name , concat(name,' ',last_name) as full_name, id as emp_id from emp_tbl where salary>150000 and age<18 \"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\" select *,'Kumar' as last_name , concat(name,' ',last_name) as full_name, cast(id as string) from emp_tbl where salary>150000 and age<18 \"\"\").printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19d02995-06d2-4e82-b416-05c92e3acd60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6dd3425-fb9d-4b2c-8d69-9ca2dcfbcf39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data=[(10 ,'Anil',50000, 18),\n",
    "(11 ,'Vikas',75000,  16),\n",
    "(12 ,'Nisha',40000,  18),\n",
    "(13 ,'Nidhi',60000,  17),\n",
    "(14 ,'Priya',80000,  18),\n",
    "(15 ,'Mohit',45000,  18),\n",
    "(16 ,'Rajesh',90000, 10),\n",
    "(17 ,'Raman',55000, 16),\n",
    "(18 ,'Sam',65000,   17)]\n",
    "\n",
    "schema = ['id','Name','sal','mngr_id']\n",
    "\n",
    "manager_df=spark.createDataFrame(data,schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a0aca7b-b986-44f4-8fb0-13535602bca8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+-------+\n| id|  Name|  sal|mngr_id|\n+---+------+-----+-------+\n| 10|  Anil|50000|     18|\n| 11| Vikas|75000|     16|\n| 12| Nisha|40000|     18|\n| 13| Nidhi|60000|     17|\n| 14| Priya|80000|     18|\n| 15| Mohit|45000|     18|\n| 16|Rajesh|90000|     10|\n| 17| Raman|55000|     16|\n| 18|   Sam|65000|     17|\n+---+------+-----+-------+\n\n"
     ]
    }
   ],
   "source": [
    "manager_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff290023-79b4-4251-b497-70d46a36110e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[3]: 9"
     ]
    }
   ],
   "source": [
    "manager_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5e57362-28ba-4674-944d-614a882511e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data1=[(19 ,'Sohan',50000, 18),\n",
    "(20 ,'Sima',75000,  17),\n",
    "(20 ,'Sima',75000,  17),(20 ,'Sima',75000,  17)]\n",
    "\n",
    "schema1 = ['id','Name','sal','mngr_id']\n",
    "\n",
    "manager_df1=spark.createDataFrame(data1,schema1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4c3201c-70f3-4ac6-9bcf-658132e8ef93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+-------+\n| id| Name|  sal|mngr_id|\n+---+-----+-----+-------+\n| 19|Sohan|50000|     18|\n| 20| Sima|75000|     17|\n| 20| Sima|75000|     17|\n| 20| Sima|75000|     17|\n+---+-----+-----+-------+\n\n"
     ]
    }
   ],
   "source": [
    "manager_df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e33c5735-9b08-474d-a311-de2419b46a1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+-------+\n| id|  Name|  sal|mngr_id|\n+---+------+-----+-------+\n| 10|  Anil|50000|     18|\n| 11| Vikas|75000|     16|\n| 12| Nisha|40000|     18|\n| 13| Nidhi|60000|     17|\n| 14| Priya|80000|     18|\n| 15| Mohit|45000|     18|\n| 16|Rajesh|90000|     10|\n| 17| Raman|55000|     16|\n| 18|   Sam|65000|     17|\n| 19| Sohan|50000|     18|\n| 20|  Sima|75000|     17|\n| 20|  Sima|75000|     17|\n| 20|  Sima|75000|     17|\n+---+------+-----+-------+\n\n"
     ]
    }
   ],
   "source": [
    "manager_df.union(manager_df1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65fb8539-6a2e-4a10-ace4-036ea8ccbc4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[11]: 13"
     ]
    }
   ],
   "source": [
    "manager_df.union(manager_df1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8774b248-198a-414d-aaf8-89b90d720399",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[12]: 13"
     ]
    }
   ],
   "source": [
    "manager_df.unionAll(manager_df1).count() # No diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d74fcbb-5bd9-4b45-bc86-66cf7523ea84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "manager_df.createOrReplaceTempView('mngr_tbl')\n",
    "\n",
    "manager_df1.createOrReplaceTempView('mngr_tbl1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6b88fb8-7171-43b1-a717-c78329d22e5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[15]: 11"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select * from mngr_tbl\n",
    "          union\n",
    "          select * from mngr_tbl1 \"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7b150b8-3363-4eac-a950-c7f7e6d173ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[17]: 13"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select * from mngr_tbl\n",
    "          union all\n",
    "          select * from mngr_tbl1 \"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcbb72c3-55a1-48d0-b0fb-974071a9dbb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[19]: 2"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select * from mngr_tbl1\n",
    "          union \n",
    "          select * from mngr_tbl1 \"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4a953d8-4373-461f-9825-c02b88b237f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "wrong_column_data=[(19 ,50000, 18,'Sohan'),\n",
    "(20 ,75000,  17,'Sima')]\n",
    "\n",
    "wrong_schema = ['id','sal','mngr_id','Name']\n",
    "\n",
    "wrong_manager_df=spark.createDataFrame(wrong_column_data,wrong_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f6ae25e-dc65-4945-ad44-bb3cb662ee35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+-------+\n| id| Name|  sal|mngr_id|\n+---+-----+-----+-------+\n| 19|Sohan|50000|     18|\n| 20| Sima|75000|     17|\n| 20| Sima|75000|     17|\n| 20| Sima|75000|     17|\n| 19|50000|   18|  Sohan|\n| 20|75000|   17|   Sima|\n+---+-----+-----+-------+\n\n"
     ]
    }
   ],
   "source": [
    "manager_df1.union(wrong_manager_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d59befc-9327-4aa0-aa81-1dba1aabb86a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "wrong_column_data=[(19 ,50000, 18,'Sohan',10),\n",
    "(20 ,75000,  17,'Sima',20)]\n",
    "\n",
    "wrong_schema = ['id','sal','mngr_id','Name','bonus']\n",
    "\n",
    "wrong_manager_df=spark.createDataFrame(wrong_column_data,wrong_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1765e5a-64d0-48e6-b316-4f34c66b347a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4050835918467517>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mmanager_df1\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munion\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwrong_manager_df\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3646\u001B[0m, in \u001B[0;36mDataFrame.union\u001B[0;34m(self, other)\u001B[0m\n",
       "\u001B[1;32m   3598\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21munion\u001B[39m(\u001B[38;5;28mself\u001B[39m, other: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[1;32m   3599\u001B[0m     \u001B[38;5;124;03m\"\"\"Return a new :class:`DataFrame` containing union of rows in this and another\u001B[39;00m\n",
       "\u001B[1;32m   3600\u001B[0m \u001B[38;5;124;03m    :class:`DataFrame`.\u001B[39;00m\n",
       "\u001B[1;32m   3601\u001B[0m \n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   3644\u001B[0m \u001B[38;5;124;03m    +----+----+----+\u001B[39;00m\n",
       "\u001B[1;32m   3645\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 3646\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munion\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [NUM_COLUMNS_MISMATCH] UNION can only be performed on inputs with the same number of columns, but the first input has 4 columns and the second input has 5 columns.;\n",
       "'Union false, false\n",
       ":- LogicalRDD [id#383L, Name#384, sal#385L, mngr_id#386L], false\n",
       "+- LogicalRDD [id#442L, sal#443L, mngr_id#444L, Name#445, bonus#446L], false\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-4050835918467517>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mmanager_df1\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munion\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwrong_manager_df\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3646\u001B[0m, in \u001B[0;36mDataFrame.union\u001B[0;34m(self, other)\u001B[0m\n\u001B[1;32m   3598\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21munion\u001B[39m(\u001B[38;5;28mself\u001B[39m, other: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   3599\u001B[0m     \u001B[38;5;124;03m\"\"\"Return a new :class:`DataFrame` containing union of rows in this and another\u001B[39;00m\n\u001B[1;32m   3600\u001B[0m \u001B[38;5;124;03m    :class:`DataFrame`.\u001B[39;00m\n\u001B[1;32m   3601\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3644\u001B[0m \u001B[38;5;124;03m    +----+----+----+\u001B[39;00m\n\u001B[1;32m   3645\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 3646\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munion\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [NUM_COLUMNS_MISMATCH] UNION can only be performed on inputs with the same number of columns, but the first input has 4 columns and the second input has 5 columns.;\n'Union false, false\n:- LogicalRDD [id#383L, Name#384, sal#385L, mngr_id#386L], false\n+- LogicalRDD [id#442L, sal#443L, mngr_id#444L, Name#445, bonus#446L], false\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [NUM_COLUMNS_MISMATCH] UNION can only be performed on inputs with the same number of columns, but the first input has 4 columns and the second input has 5 columns.;\n'Union false, false\n:- LogicalRDD [id#383L, Name#384, sal#385L, mngr_id#386L], false\n+- LogicalRDD [id#442L, sal#443L, mngr_id#444L, Name#445, bonus#446L], false\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "manager_df1.union(wrong_manager_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16d16c8c-3128-479d-9c4a-62122a5d1ff1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+-----+\n| id|  sal|mngr_id| Name|\n+---+-----+-------+-----+\n| 19|50000|     18|Sohan|\n| 20|75000|     17| Sima|\n| 19|Sohan|  50000|   18|\n| 20| Sima|  75000|   17|\n| 20| Sima|  75000|   17|\n| 20| Sima|  75000|   17|\n+---+-----+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "wrong_manager_df.select('id','sal','mngr_id','Name').union(manager_df1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ea7c506-9ef8-4f36-a4c0-3c4ca450df03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+-------+\n| id| Name|  sal|mngr_id|\n+---+-----+-----+-------+\n| 19|Sohan|50000|     18|\n| 20| Sima|75000|     17|\n| 20| Sima|75000|     17|\n| 20| Sima|75000|     17|\n| 19|Sohan|50000|     18|\n| 20| Sima|75000|     17|\n+---+-----+-----+-------+\n\n"
     ]
    }
   ],
   "source": [
    "manager_df1.unionByName(wrong_manager_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a749df1-7fc3-4449-893a-4ed590765176",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "wrong_column_data1=[(19 ,50000, 18,'Sohan'),\n",
    "(20 ,75000,  17,'Sima')]\n",
    "\n",
    "wrong_schema1 = ['id','sal','mngr_id','Nam']\n",
    "\n",
    "wrong_manager_df1=spark.createDataFrame(wrong_column_data1,wrong_schema1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcae9c56-e94f-4bc3-8803-afd6730f1fb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4270033757460263>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mwrong_manager_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munionByName\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwrong_manager_df1\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3738\u001B[0m, in \u001B[0;36mDataFrame.unionByName\u001B[0;34m(self, other, allowMissingColumns)\u001B[0m\n",
       "\u001B[1;32m   3682\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21munionByName\u001B[39m(\u001B[38;5;28mself\u001B[39m, other: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m, allowMissingColumns: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[1;32m   3683\u001B[0m     \u001B[38;5;124;03m\"\"\"Returns a new :class:`DataFrame` containing union of rows in this and another\u001B[39;00m\n",
       "\u001B[1;32m   3684\u001B[0m \u001B[38;5;124;03m    :class:`DataFrame`.\u001B[39;00m\n",
       "\u001B[1;32m   3685\u001B[0m \n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   3736\u001B[0m \u001B[38;5;124;03m    +----+----+----+----+\u001B[39;00m\n",
       "\u001B[1;32m   3737\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 3738\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munionByName\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallowMissingColumns\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Cannot resolve column name \"Name\" among (id, sal, mngr_id, Nam)."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-4270033757460263>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mwrong_manager_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munionByName\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwrong_manager_df1\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3738\u001B[0m, in \u001B[0;36mDataFrame.unionByName\u001B[0;34m(self, other, allowMissingColumns)\u001B[0m\n\u001B[1;32m   3682\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21munionByName\u001B[39m(\u001B[38;5;28mself\u001B[39m, other: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m, allowMissingColumns: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   3683\u001B[0m     \u001B[38;5;124;03m\"\"\"Returns a new :class:`DataFrame` containing union of rows in this and another\u001B[39;00m\n\u001B[1;32m   3684\u001B[0m \u001B[38;5;124;03m    :class:`DataFrame`.\u001B[39;00m\n\u001B[1;32m   3685\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3736\u001B[0m \u001B[38;5;124;03m    +----+----+----+----+\u001B[39;00m\n\u001B[1;32m   3737\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 3738\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munionByName\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallowMissingColumns\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Cannot resolve column name \"Name\" among (id, sal, mngr_id, Nam).",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Cannot resolve column name \"Name\" among (id, sal, mngr_id, Nam).",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "wrong_manager_df.unionByName(wrong_manager_df1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a760099a-7e0c-48a0-8feb-4e3d0fc1b8cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "If else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "425f3521-a1eb-4ce4-97ad-078669a6b8b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp_data = [\n",
    "(1,'manish',26,20000,'india','IT'),\n",
    "(2,'rahul',None,40000,'germany','engineering'),\n",
    "(3,'pawan',12,60000,'india','sales'),\n",
    "(4,'roshini',44,None,'uk','engineering'),\n",
    "(5,'raushan',35,70000,'india','sales'),\n",
    "(6,None,29,200000,'uk','IT'),\n",
    "(7,'adam',37,65000,'us','IT'),\n",
    "(8,'chris',16,40000,'us','sales'),\n",
    "(None,None,None,None,None,None),\n",
    "(7,'adam',37,65000,'us','IT')\n",
    "]\n",
    "\n",
    "schema=['id','name','age','salary','country','dept']\n",
    "\n",
    "emp_df=spark.createDataFrame(emp_data,schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e77bea6f-c266-45c4-ab78-81c78151416c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+------+-------+-----------+\n|  id|   name| age|salary|country|       dept|\n+----+-------+----+------+-------+-----------+\n|   1| manish|  26| 20000|  india|         IT|\n|   2|  rahul|null| 40000|germany|engineering|\n|   3|  pawan|  12| 60000|  india|      sales|\n|   4|roshini|  44|  null|     uk|engineering|\n|   5|raushan|  35| 70000|  india|      sales|\n|   6|   null|  29|200000|     uk|         IT|\n|   7|   adam|  37| 65000|     us|         IT|\n|   8|  chris|  16| 40000|     us|      sales|\n|null|   null|null|  null|   null|       null|\n|   7|   adam|  37| 65000|     us|         IT|\n+----+-------+----+------+-------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7732c40b-439d-4a12-b0f1-b2581c888f70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3acacf69-7c5b-47ac-8112-f96d4979f587",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+------+-------+-----------+-------+\n|  id|   name| age|salary|country|       dept|  adult|\n+----+-------+----+------+-------+-----------+-------+\n|   1| manish|  26| 20000|  india|         IT|    Yes|\n|   2|  rahul|null| 40000|germany|engineering|NoValue|\n|   3|  pawan|  12| 60000|  india|      sales|     No|\n|   4|roshini|  44|  null|     uk|engineering|    Yes|\n|   5|raushan|  35| 70000|  india|      sales|    Yes|\n|   6|   null|  29|200000|     uk|         IT|    Yes|\n|   7|   adam|  37| 65000|     us|         IT|    Yes|\n|   8|  chris|  16| 40000|     us|      sales|     No|\n|null|   null|null|  null|   null|       null|NoValue|\n|   7|   adam|  37| 65000|     us|         IT|    Yes|\n+----+-------+----+------+-------+-----------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.withColumn(\"adult\",when(col('age')<18,'No')\n",
    "                  .when(col('age')>18,'Yes')\n",
    "                  .otherwise('NoValue')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd0f994c-41f5-431c-87de-9d2c50f2674c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+------+-------+-----------+-----+\n|  id|   name|age|salary|country|       dept|adult|\n+----+-------+---+------+-------+-----------+-----+\n|   1| manish| 26| 20000|  india|         IT|  Yes|\n|   2|  rahul| 19| 40000|germany|engineering|  Yes|\n|   3|  pawan| 12| 60000|  india|      sales|   No|\n|   4|roshini| 44|  null|     uk|engineering|  Yes|\n|   5|raushan| 35| 70000|  india|      sales|  Yes|\n|   6|   null| 29|200000|     uk|         IT|  Yes|\n|   7|   adam| 37| 65000|     us|         IT|  Yes|\n|   8|  chris| 16| 40000|     us|      sales|   No|\n|null|   null| 19|  null|   null|       null|  Yes|\n|   7|   adam| 37| 65000|     us|         IT|  Yes|\n+----+-------+---+------+-------+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.withColumn('age',when(col('age').isNull(),lit(19)).otherwise(col('age')))\\\n",
    ".withColumn(\"adult\",when(col('age')<18,'No').otherwise('Yes')\n",
    "                  ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fee6de5f-382b-45b5-8ad0-80dc69cce8a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPy4JError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1497892530559978>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m emp_df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mage_wise\u001B[39m\u001B[38;5;124m'\u001B[39m,when(col(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mage\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m>\u001B[39m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m&\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mcol\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mage\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m<\u001B[39m\u001B[38;5;241m18\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMinor\u001B[39m\u001B[38;5;124m'\u001B[39m))\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/column.py:164\u001B[0m, in \u001B[0;36m_bin_op.<locals>._\u001B[0;34m(self, other)\u001B[0m\n",
       "\u001B[1;32m    159\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_\u001B[39m(\n",
       "\u001B[1;32m    160\u001B[0m     \u001B[38;5;28mself\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumn\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    161\u001B[0m     other: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumn\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLiteralType\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDecimalLiteral\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDateTimeLiteral\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n",
       "\u001B[1;32m    162\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumn\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[1;32m    163\u001B[0m     jc \u001B[38;5;241m=\u001B[39m other\u001B[38;5;241m.\u001B[39m_jc \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(other, Column) \u001B[38;5;28;01melse\u001B[39;00m other\n",
       "\u001B[0;32m--> 164\u001B[0m     njc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43mjc\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    165\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Column(njc)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n",
       "\u001B[1;32m    227\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 228\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    230\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:330\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n",
       "\u001B[1;32m    326\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n",
       "\u001B[1;32m    327\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    328\u001B[0m             \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n",
       "\u001B[1;32m    329\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 330\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n",
       "\u001B[1;32m    331\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    332\u001B[0m             \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
       "\u001B[1;32m    333\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    334\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n",
       "\u001B[1;32m    335\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    336\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name))\n",
       "\n",
       "\u001B[0;31mPy4JError\u001B[0m: An error occurred while calling o1641.and. Trace:\n",
       "py4j.Py4JException: Method and([class java.lang.Integer]) does not exist\n",
       "\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:341)\n",
       "\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:349)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:297)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-1497892530559978>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m emp_df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mage_wise\u001B[39m\u001B[38;5;124m'\u001B[39m,when(col(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mage\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m>\u001B[39m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m&\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mcol\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mage\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m<\u001B[39m\u001B[38;5;241m18\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMinor\u001B[39m\u001B[38;5;124m'\u001B[39m))\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/column.py:164\u001B[0m, in \u001B[0;36m_bin_op.<locals>._\u001B[0;34m(self, other)\u001B[0m\n\u001B[1;32m    159\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_\u001B[39m(\n\u001B[1;32m    160\u001B[0m     \u001B[38;5;28mself\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumn\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    161\u001B[0m     other: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumn\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLiteralType\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDecimalLiteral\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDateTimeLiteral\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    162\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumn\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    163\u001B[0m     jc \u001B[38;5;241m=\u001B[39m other\u001B[38;5;241m.\u001B[39m_jc \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(other, Column) \u001B[38;5;28;01melse\u001B[39;00m other\n\u001B[0;32m--> 164\u001B[0m     njc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43mjc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    165\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Column(njc)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    227\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 228\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    230\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:330\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    326\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m             \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 330\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m             \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n\u001B[1;32m    333\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    334\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    335\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    336\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name))\n\n\u001B[0;31mPy4JError\u001B[0m: An error occurred while calling o1641.and. Trace:\npy4j.Py4JException: Method and([class java.lang.Integer]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:341)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:349)\n\tat py4j.Gateway.invoke(Gateway.java:297)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\n\n",
       "errorSummary": "py4j.Py4JException: Method and([class java.lang.Integer]) does not exist",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "emp_df.withColumn('age_wise',when(col('age')>0 & col('age')<18,'Minor')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49ed6bef-51c4-49fc-9197-68dff2d95e0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+------+-------+-----------+--------+\n|  id|   name| age|salary|country|       dept|age_wise|\n+----+-------+----+------+-------+-----------+--------+\n|   1| manish|  26| 20000|  india|         IT|     Mid|\n|   2|  rahul|null| 40000|germany|engineering|   Major|\n|   3|  pawan|  12| 60000|  india|      sales|   Minor|\n|   4|roshini|  44|  null|     uk|engineering|   Major|\n|   5|raushan|  35| 70000|  india|      sales|   Major|\n|   6|   null|  29|200000|     uk|         IT|     Mid|\n|   7|   adam|  37| 65000|     us|         IT|   Major|\n|   8|  chris|  16| 40000|     us|      sales|   Minor|\n|null|   null|null|  null|   null|       null|   Major|\n|   7|   adam|  37| 65000|     us|         IT|   Major|\n+----+-------+----+------+-------+-----------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.withColumn('age_wise',when((col('age')>0) & (col('age')<18),'Minor')\n",
    "                  .when((col('age')>18) & (col('age')<30),'Mid')\n",
    "                  .otherwise('Major'))\\\n",
    "                  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4ca91ff-3d6f-4ff8-b3f0-0ff4d4ca251d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp_df.createOrReplaceTempView('emp_tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97ec905d-8ec2-4ded-8ada-f89d252bde5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+------+-------+-----------+-------+\n|  id|   name| age|salary|country|       dept|  adult|\n+----+-------+----+------+-------+-----------+-------+\n|   1| manish|  26| 20000|  india|         IT|  Major|\n|   2|  rahul|null| 40000|germany|engineering|NoValue|\n|   3|  pawan|  12| 60000|  india|      sales|  Minor|\n|   4|roshini|  44|  null|     uk|engineering|  Major|\n|   5|raushan|  35| 70000|  india|      sales|  Major|\n|   6|   null|  29|200000|     uk|         IT|  Major|\n|   7|   adam|  37| 65000|     us|         IT|  Major|\n|   8|  chris|  16| 40000|     us|      sales|  Minor|\n|null|   null|null|  null|   null|       null|NoValue|\n|   7|   adam|  37| 65000|     us|         IT|  Major|\n+----+-------+----+------+-------+-----------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "          select *,\n",
    "          case when age<18 then 'Minor'\n",
    "          when age>18 then 'Major'\n",
    "          else 'NoValue'\n",
    "          end as adult\n",
    "          from emp_tbl\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f3c1a6e-5a5c-4720-9709-8750e4e289bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data=[(10 ,'Anil',50000, 18),\n",
    "(11 ,'Vikas',75000,  16),\n",
    "(12 ,'Nisha',40000,  18),\n",
    "(13 ,'Nidhi',60000,  17),\n",
    "(14 ,'Priya',80000,  18),\n",
    "(15 ,'Mohit',45000,  18),\n",
    "(16 ,'Rajesh',90000, 10),\n",
    "(17 ,'Raman',55000, 16),\n",
    "(18 ,'Sam',65000,   17),\n",
    "(15 ,'Mohit',45000,  18),\n",
    "(13 ,'Nidhi',60000,  17),      \n",
    "(14 ,'Priya',90000,  18),  \n",
    "(18 ,'Sam',65000,   17)\n",
    "     ]\n",
    "schema = ['id','Name','sal','mngr_id']\n",
    "\n",
    "manager_df=spark.createDataFrame(data,schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3785f478-be87-4427-9aa9-483aa45acc7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+-------+\n| id|  Name|  sal|mngr_id|\n+---+------+-----+-------+\n| 10|  Anil|50000|     18|\n| 11| Vikas|75000|     16|\n| 12| Nisha|40000|     18|\n| 13| Nidhi|60000|     17|\n| 14| Priya|80000|     18|\n| 15| Mohit|45000|     18|\n| 16|Rajesh|90000|     10|\n| 17| Raman|55000|     16|\n| 18|   Sam|65000|     17|\n| 15| Mohit|45000|     18|\n| 13| Nidhi|60000|     17|\n| 14| Priya|90000|     18|\n| 18|   Sam|65000|     17|\n+---+------+-----+-------+\n\n"
     ]
    }
   ],
   "source": [
    "manager_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60336e8e-9693-4438-bd4f-7b5ecf59b62e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[44]: 13"
     ]
    }
   ],
   "source": [
    "manager_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bfc99ab-500d-4753-a96c-f0f19db7fb4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+-------+\n| id|  Name|  sal|mngr_id|\n+---+------+-----+-------+\n| 10|  Anil|50000|     18|\n| 12| Nisha|40000|     18|\n| 11| Vikas|75000|     16|\n| 13| Nidhi|60000|     17|\n| 15| Mohit|45000|     18|\n| 14| Priya|80000|     18|\n| 16|Rajesh|90000|     10|\n| 17| Raman|55000|     16|\n| 18|   Sam|65000|     17|\n| 14| Priya|90000|     18|\n+---+------+-----+-------+\n\n"
     ]
    }
   ],
   "source": [
    "manager_df.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "486587fd-397e-4f02-a133-de68c93e2c4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[46]: 10"
     ]
    }
   ],
   "source": [
    "manager_df.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60aedbf1-d73f-40cb-9062-a1b2cfed338c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2005587586478558>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mmanager_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdistinct\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mid\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mname\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: distinct() takes 1 positional argument but 3 were given"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-2005587586478558>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mmanager_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdistinct\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mid\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mname\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\n\u001B[0;31mTypeError\u001B[0m: distinct() takes 1 positional argument but 3 were given",
       "errorSummary": "<span class='ansi-red-fg'>TypeError</span>: distinct() takes 1 positional argument but 3 were given",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "manager_df.distinct('id','name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1225864-27b8-4db9-93c6-ea4acc11972c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n| id|  name|\n+---+------+\n| 10|  Anil|\n| 11| Vikas|\n| 12| Nisha|\n| 13| Nidhi|\n| 15| Mohit|\n| 14| Priya|\n| 17| Raman|\n| 16|Rajesh|\n| 18|   Sam|\n+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "manager_df.select('id','name').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adccf28f-4578-4eca-aada-1be7fbabc4b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+-------+\n| id|  Name|  sal|mngr_id|\n+---+------+-----+-------+\n| 10|  Anil|50000|     18|\n| 12| Nisha|40000|     18|\n| 11| Vikas|75000|     16|\n| 13| Nidhi|60000|     17|\n| 15| Mohit|45000|     18|\n| 14| Priya|80000|     18|\n| 16|Rajesh|90000|     10|\n| 17| Raman|55000|     16|\n| 18|   Sam|65000|     17|\n| 14| Priya|90000|     18|\n+---+------+-----+-------+\n\n"
     ]
    }
   ],
   "source": [
    "manager_df.drop_duplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49eac61d-0d17-4713-98fe-6a649c931445",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+-------+\n| id|  Name|  sal|mngr_id|\n+---+------+-----+-------+\n| 10|  Anil|50000|     18|\n| 12| Nisha|40000|     18|\n| 11| Vikas|75000|     16|\n| 13| Nidhi|60000|     17|\n| 15| Mohit|45000|     18|\n| 14| Priya|80000|     18|\n| 16|Rajesh|90000|     10|\n| 17| Raman|55000|     16|\n| 18|   Sam|65000|     17|\n| 14| Priya|90000|     18|\n+---+------+-----+-------+\n\n"
     ]
    }
   ],
   "source": [
    "manager_df.dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a2f073f-1742-4f1b-8517-94bb8342b501",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dropped_mngr_df = manager_df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a4de77e-8f0f-43b3-ac13-a6d1457b44ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+-------+\n| id|  Name|  sal|mngr_id|\n+---+------+-----+-------+\n| 12| Nisha|40000|     18|\n| 15| Mohit|45000|     18|\n| 15| Mohit|45000|     18|\n| 10|  Anil|50000|     18|\n| 17| Raman|55000|     16|\n| 13| Nidhi|60000|     17|\n| 13| Nidhi|60000|     17|\n| 18|   Sam|65000|     17|\n| 18|   Sam|65000|     17|\n| 11| Vikas|75000|     16|\n| 14| Priya|80000|     18|\n| 16|Rajesh|90000|     10|\n| 14| Priya|90000|     18|\n+---+------+-----+-------+\n\n"
     ]
    }
   ],
   "source": [
    "manager_df.sort('sal').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cafebae-1a68-400e-9d52-6305cee104c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+-------+\n| id|  Name|  sal|mngr_id|\n+---+------+-----+-------+\n| 14| Priya|90000|     18|\n| 16|Rajesh|90000|     10|\n| 14| Priya|80000|     18|\n| 11| Vikas|75000|     16|\n| 18|   Sam|65000|     17|\n| 18|   Sam|65000|     17|\n| 13| Nidhi|60000|     17|\n| 13| Nidhi|60000|     17|\n| 17| Raman|55000|     16|\n| 10|  Anil|50000|     18|\n| 15| Mohit|45000|     18|\n| 15| Mohit|45000|     18|\n| 12| Nisha|40000|     18|\n+---+------+-----+-------+\n\n"
     ]
    }
   ],
   "source": [
    "manager_df.sort(col('sal').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81f4b4ab-e500-4532-8cf2-a2ca97c521b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+-------+\n| id|  Name|  sal|mngr_id|\n+---+------+-----+-------+\n| 16|Rajesh|90000|     10|\n| 14| Priya|90000|     18|\n| 14| Priya|80000|     18|\n| 11| Vikas|75000|     16|\n| 18|   Sam|65000|     17|\n| 18|   Sam|65000|     17|\n| 13| Nidhi|60000|     17|\n| 13| Nidhi|60000|     17|\n| 17| Raman|55000|     16|\n| 10|  Anil|50000|     18|\n| 15| Mohit|45000|     18|\n| 15| Mohit|45000|     18|\n| 12| Nisha|40000|     18|\n+---+------+-----+-------+\n\n"
     ]
    }
   ],
   "source": [
    "manager_df.sort(col('sal').desc(),col('Name').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03e594ae-4fd6-43de-ad6a-3e55ed015584",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "leet_code_data = [\n",
    "    (1, 'Will', None),\n",
    "    (2, 'Jane', None),\n",
    "    (3, 'Alex', 2),\n",
    "    (4, 'Bill', None),\n",
    "    (5, 'Zack', 1),\n",
    "    (6, 'Mark', 2)\n",
    "]\n",
    "\n",
    "leet_code_schema = ['id','name','referee_id']\n",
    "\n",
    "cust_df = spark.createDataFrame(leet_code_data,leet_code_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0decb686-6ab2-4f80-a0e7-4301993b204c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------+\n| id|name|referee_id|\n+---+----+----------+\n|  1|Will|      null|\n|  2|Jane|      null|\n|  3|Alex|         2|\n|  4|Bill|      null|\n|  5|Zack|         1|\n|  6|Mark|         2|\n+---+----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "cust_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17658975-66a7-4aad-a1aa-d1284fde5434",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n|name|\n+----+\n|Will|\n|Jane|\n|Bill|\n|Zack|\n+----+\n\n"
     ]
    }
   ],
   "source": [
    "cust_df.select('name').where((col('referee_id') != 2) |(col('referee_id').isNull())) .show() # where referee_id is not 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5ac0895-109e-4be2-b741-5811baf5d185",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Aggregate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e1affc1-f54a-4ede-83d5-df18f2190fe7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c955912d-9a1e-4693-b863-e57351ed490a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp_data = [\n",
    "(1,'manish',26,20000,'india','IT'),\n",
    "(2,'rahul',None,40000,'germany','engineering'),\n",
    "(3,'pawan',12,60000,'india','sales'),\n",
    "(4,'roshini',44,None,'uk','engineering'),\n",
    "(5,'raushan',35,70000,'india','sales'),\n",
    "(6,None,29,200000,'uk','IT'),\n",
    "(7,'adam',37,65000,'us','IT'),\n",
    "(8,'chris',16,40000,'us','sales'),\n",
    "(None,None,None,None,None,None),\n",
    "(7,'adam',37,65000,'us','IT')\n",
    "]\n",
    "\n",
    "emp_schema = ['id','name','age','salary','country','dept']\n",
    "\n",
    "emp_df=spark.createDataFrame(emp_data,emp_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2abd78c3-6822-4c38-8652-6dd3618e7207",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+------+-------+-----------+\n|  id|   name| age|salary|country|       dept|\n+----+-------+----+------+-------+-----------+\n|   1| manish|  26| 20000|  india|         IT|\n|   2|  rahul|null| 40000|germany|engineering|\n|   3|  pawan|  12| 60000|  india|      sales|\n|   4|roshini|  44|  null|     uk|engineering|\n|   5|raushan|  35| 70000|  india|      sales|\n|   6|   null|  29|200000|     uk|         IT|\n|   7|   adam|  37| 65000|     us|         IT|\n|   8|  chris|  16| 40000|     us|      sales|\n|null|   null|null|  null|   null|       null|\n|   7|   adam|  37| 65000|     us|         IT|\n+----+-------+----+------+-------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d19c4af3-2157-4155-b0fb-eb7345142070",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[3]: 10"
     ]
    }
   ],
   "source": [
    "emp_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83e3af17-a788-45b5-b8f2-282d133046ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|count(name)|\n+-----------+\n|          8|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.select(count('name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3151efdc-713a-493f-bd91-338e121ac8d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n|count(id)|\n+---------+\n|        9|\n+---------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.select(count('id')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7955a79c-8e3f-49f3-a6e4-fdee4c0e367a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|count(1)|\n+--------+\n|      10|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.select(count('*')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f1f8eca-3671-4999-9b3c-298f4a8bec9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----------+-----------+\n|sum(salary)|min(salary)|max(salary)|avg(salary)|\n+-----------+-----------+-----------+-----------+\n|     560000|      20000|     200000|    70000.0|\n+-----------+-----------+-----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.select(sum('salary'),min('salary'),max('salary'),avg('salary')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7747115c-2c7b-4415-a6a4-9d9d65983cd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+--------------+--------------+\n|total_salary|minimum_salary|maximum_salary|average_salary|\n+------------+--------------+--------------+--------------+\n|      560000|         20000|        200000|       70000.0|\n+------------+--------------+--------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.select(sum('salary').alias('total_salary'),min('salary').alias('minimum_salary'),max('salary').alias('maximum_salary'),avg('salary').alias('average_salary')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77014676-20e8-4d2f-8ae7-a0f38d23cfe5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+--------------+--------------+\n|total_salary|minimum_salary|maximum_salary|average_salary|\n+------------+--------------+--------------+--------------+\n|      560000|         20000|        200000|         70000|\n+------------+--------------+--------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.select(sum('salary').alias('total_salary'),min('salary').alias('minimum_salary'),max('salary').alias('maximum_salary'),avg('salary').cast('integer').alias('average_salary')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f06847e-0beb-496c-8f32-731ffe77a15a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Group By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7fdc149-891e-4e6f-92ab-d1e913ee6840",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+---------+\n| id|   name|salary|     dept|\n+---+-------+------+---------+\n|  1| manish| 50000|       IT|\n|  2| vikash| 60000|    sales|\n|  3|raushan| 70000|marketing|\n|  4| mukesh| 80000|       IT|\n|  5| pritam| 90000|    sales|\n|  6| nikita| 45000|marketing|\n|  7| ragini| 55000|marketing|\n|  8| rakesh|100000|       IT|\n|  9| aditya| 65000|       IT|\n| 10|  rahul| 50000|marketing|\n+---+-------+------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_data=[(1,'manish',50000,\"IT\"),\n",
    "(2,'vikash',60000,'sales'),\n",
    "(3,'raushan',70000,'marketing'),\n",
    "(4,'mukesh',80000,'IT'),\n",
    "(5,'pritam',90000,'sales'),\n",
    "(6,'nikita',45000,'marketing'),\n",
    "(7,'ragini',55000,'marketing'),\n",
    "(8,'rakesh',100000,'IT'),\n",
    "(9,'aditya',65000,'IT'),\n",
    "(10,'rahul',50000,'marketing')]\n",
    "\n",
    "emp_schema = ['id','name','salary','dept']\n",
    "\n",
    "emp_df=spark.createDataFrame(emp_data,emp_schema)\n",
    "\n",
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "224b9ce7-23e9-4b1b-96f5-de67c012fc44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------+-----------------+-----+\n|summary|                id|  name|           salary| dept|\n+-------+------------------+------+-----------------+-----+\n|  count|                10|    10|               10|   10|\n|   mean|               5.5|  null|          66500.0| null|\n| stddev|3.0276503540974917|  null|18416.47812874836| null|\n|    min|                 1|aditya|            45000|   IT|\n|    max|                10|vikash|           100000|sales|\n+-------+------------------+------+-----------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d77e43dc-a7a0-4091-a6a1-85755b8aa691",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n|     dept|sum(salary)|\n+---------+-----------+\n|       IT|     295000|\n|    sales|     150000|\n|marketing|     220000|\n+---------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.groupBy('dept')\\\n",
    "    .agg(sum('salary')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd57ec38-8d4b-4dd5-ad0f-adb3de10db36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+---------+-------+\n| id|   name|salary|     dept|country|\n+---+-------+------+---------+-------+\n|  1| manish| 50000|       IT|  india|\n|  2| vikash| 60000|    sales|     us|\n|  3|raushan| 70000|marketing|  india|\n|  4| mukesh| 80000|       IT|     us|\n|  5| pritam| 90000|    sales|  india|\n|  6| nikita| 45000|marketing|     us|\n|  7| ragini| 55000|marketing|  india|\n|  8| rakesh|100000|       IT|     us|\n|  9| aditya| 65000|       IT|  india|\n| 10|  rahul| 50000|marketing|     us|\n+---+-------+------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_data=[(1,'manish',50000,\"IT\",'india'),\n",
    "(2,'vikash',60000,'sales','us'),\n",
    "(3,'raushan',70000,'marketing','india'),\n",
    "(4,'mukesh',80000,'IT','us'),\n",
    "(5,'pritam',90000,'sales','india'),\n",
    "(6,'nikita',45000,'marketing','us'),\n",
    "(7,'ragini',55000,'marketing','india'),\n",
    "(8,'rakesh',100000,'IT','us'),\n",
    "(9,'aditya',65000,'IT','india'),\n",
    "(10,'rahul',50000,'marketing','us')]\n",
    "\n",
    "emp_schema = ['id','name','salary','dept','country']\n",
    "\n",
    "emp_df=spark.createDataFrame(emp_data,emp_schema)\n",
    "\n",
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ff8bd57-a141-4dfe-b017-7e26ea6b4ef3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-----------+\n|     dept|country|sum(salary)|\n+---------+-------+-----------+\n|       IT|  india|     115000|\n|       IT|     us|     180000|\n|marketing|  india|     125000|\n|marketing|     us|      95000|\n|    sales|  india|      90000|\n|    sales|     us|      60000|\n+---------+-------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.groupBy('dept','country')\\\n",
    "    .agg(sum('salary')).sort('dept','country').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f8c533d-f5e4-4625-a871-98985de36c0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp_df.createOrReplaceTempView('tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3e03d1d-e882-44e1-ac7f-f608837cb45e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n|     dept|sum(salary)|\n+---------+-----------+\n|       IT|     295000|\n|    sales|     150000|\n|marketing|     220000|\n+---------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select dept, SUM(salary) from tbl group by dept \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d1396a6-4270-4af1-95d4-27b3bc723bbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "995c47a9-68fb-48e9-961f-f1cedb446676",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_data = [(1,'manish','patna',\"30-05-2022\"),\n",
    "(2,'vikash','kolkata',\"12-03-2023\"),\n",
    "(3,'nikita','delhi',\"25-06-2023\"),\n",
    "(4,'rahul','ranchi',\"24-03-2023\"),\n",
    "(5,'mahesh','jaipur',\"22-03-2023\"),\n",
    "(6,'prantosh','kolkata',\"18-10-2022\"),\n",
    "(7,'raman','patna',\"30-12-2022\"),\n",
    "(8,'prakash','ranchi',\"24-02-2023\"),\n",
    "(9,'ragini','kolkata',\"03-03-2023\"),\n",
    "(10,'raushan','jaipur',\"05-02-2023\")]\n",
    "\n",
    "customer_schema=['customer_id','customer_name','address','date_of_joining']\n",
    "\n",
    "customer_df=spark.createDataFrame(customer_data,customer_schema)\n",
    "\n",
    "customer_df.createOrReplaceTempView('cust_tbl')\n",
    "\n",
    "sales_data = [(1,22,10,\"01-06-2022\"),\n",
    "(1,27,5,\"03-02-2023\"),\n",
    "(2,5,3,\"01-06-2023\"),\n",
    "(5,22,1,\"22-03-2023\"),\n",
    "(7,22,4,\"03-02-2023\"),\n",
    "(9,5,6,\"03-03-2023\"),\n",
    "(2,1,12,\"15-06-2023\"),\n",
    "(1,56,2,\"25-06-2023\"),\n",
    "(5,12,5,\"15-04-2023\"),\n",
    "(11,12,76,\"12-03-2023\")]\n",
    "\n",
    "sales_schema=['customer_id','product_id','quantity','date_of_purchase']\n",
    "\n",
    "sales_df=spark.createDataFrame(sales_data,sales_schema)\n",
    "\n",
    "sales_df.createOrReplaceTempView('sales_tbl')\n",
    "\n",
    "product_data = [(1, 'fanta',20),\n",
    "(2, 'dew',22),\n",
    "(5, 'sprite',40),\n",
    "(7, 'redbull',100),\n",
    "(12,'mazza',45),\n",
    "(22,'coke',27),\n",
    "(25,'limca',21),\n",
    "(27,'pepsi',14),\n",
    "(56,'sting',10)]\n",
    "\n",
    "product_schema=['id','name','price']\n",
    "\n",
    "product_df=spark.createDataFrame(product_data,product_schema)\n",
    "\n",
    "product_df.createOrReplaceTempView('pdt_tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65c19ed4-2090-4bd6-8291-4965ecd680dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n|customer_id|customer_name|address|date_of_joining|customer_id|product_id|quantity|date_of_purchase|\n+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n|          1|       manish|  patna|     30-05-2022|          1|        22|      10|      01-06-2022|\n|          1|       manish|  patna|     30-05-2022|          1|        27|       5|      03-02-2023|\n|          1|       manish|  patna|     30-05-2022|          1|        56|       2|      25-06-2023|\n|          2|       vikash|kolkata|     12-03-2023|          2|         5|       3|      01-06-2023|\n|          2|       vikash|kolkata|     12-03-2023|          2|         1|      12|      15-06-2023|\n|          5|       mahesh| jaipur|     22-03-2023|          5|        22|       1|      22-03-2023|\n|          5|       mahesh| jaipur|     22-03-2023|          5|        12|       5|      15-04-2023|\n|          7|        raman|  patna|     30-12-2022|          7|        22|       4|      03-02-2023|\n|          9|       ragini|kolkata|     03-03-2023|          9|         5|       6|      03-03-2023|\n+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "customer_df.join(sales_df,sales_df['customer_id']==customer_df['customer_id'],'inner').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab019577-d1d9-4fff-ab8b-319bec207589",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2019211049380503>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mcustomer_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43msales_df\u001B[49m\u001B[43m,\u001B[49m\u001B[43msales_df\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcustomer_id\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43mcustomer_df\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcustomer_id\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43minner\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcustomer_id\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3023\u001B[0m, in \u001B[0;36mDataFrame.select\u001B[0;34m(self, *cols)\u001B[0m\n",
       "\u001B[1;32m   2978\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumnOrName\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n",
       "\u001B[1;32m   2979\u001B[0m     \u001B[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001B[39;00m\n",
       "\u001B[1;32m   2980\u001B[0m \n",
       "\u001B[1;32m   2981\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   3021\u001B[0m \u001B[38;5;124;03m    +-----+---+\u001B[39;00m\n",
       "\u001B[1;32m   3022\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 3023\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jcols\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcols\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   3024\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [AMBIGUOUS_REFERENCE] Reference `customer_id` is ambiguous, could be: [`customer_id`, `customer_id`]."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-2019211049380503>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mcustomer_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43msales_df\u001B[49m\u001B[43m,\u001B[49m\u001B[43msales_df\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcustomer_id\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43mcustomer_df\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcustomer_id\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43minner\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcustomer_id\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3023\u001B[0m, in \u001B[0;36mDataFrame.select\u001B[0;34m(self, *cols)\u001B[0m\n\u001B[1;32m   2978\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumnOrName\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   2979\u001B[0m     \u001B[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001B[39;00m\n\u001B[1;32m   2980\u001B[0m \n\u001B[1;32m   2981\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3021\u001B[0m \u001B[38;5;124;03m    +-----+---+\u001B[39;00m\n\u001B[1;32m   3022\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 3023\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jcols\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcols\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3024\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [AMBIGUOUS_REFERENCE] Reference `customer_id` is ambiguous, could be: [`customer_id`, `customer_id`].",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [AMBIGUOUS_REFERENCE] Reference `customer_id` is ambiguous, could be: [`customer_id`, `customer_id`].",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "customer_df.join(sales_df,sales_df['customer_id']==customer_df['customer_id'],'inner').select('customer_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93e153fe-a7c3-41a4-8c75-3ed5146a49f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|customer_id|\n+-----------+\n|          1|\n|          1|\n|          1|\n|          2|\n|          2|\n|          5|\n|          5|\n|          7|\n|          9|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "customer_df.join(sales_df,sales_df['customer_id']==customer_df['customer_id'],'inner').select(sales_df['customer_id']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b017927-32c7-4ccb-840b-03a43f49161f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n|customer_id|customer_name|address|date_of_joining|customer_id|product_id|quantity|date_of_purchase|\n+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n|          1|       manish|  patna|     30-05-2022|          1|        56|       2|      25-06-2023|\n|          1|       manish|  patna|     30-05-2022|          1|        27|       5|      03-02-2023|\n|          1|       manish|  patna|     30-05-2022|          1|        22|      10|      01-06-2022|\n|          2|       vikash|kolkata|     12-03-2023|          2|         1|      12|      15-06-2023|\n|          2|       vikash|kolkata|     12-03-2023|          2|         5|       3|      01-06-2023|\n|          3|       nikita|  delhi|     25-06-2023|       null|      null|    null|            null|\n|          4|        rahul| ranchi|     24-03-2023|       null|      null|    null|            null|\n|          5|       mahesh| jaipur|     22-03-2023|          5|        12|       5|      15-04-2023|\n|          5|       mahesh| jaipur|     22-03-2023|          5|        22|       1|      22-03-2023|\n|          6|     prantosh|kolkata|     18-10-2022|       null|      null|    null|            null|\n|          7|        raman|  patna|     30-12-2022|          7|        22|       4|      03-02-2023|\n|          8|      prakash| ranchi|     24-02-2023|       null|      null|    null|            null|\n|          9|       ragini|kolkata|     03-03-2023|          9|         5|       6|      03-03-2023|\n|         10|      raushan| jaipur|     05-02-2023|       null|      null|    null|            null|\n+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    " customer_df.join(sales_df,sales_df['customer_id']==customer_df['customer_id'],'left').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "768a4506-a2e1-4959-9014-7d344858d050",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+--------+----------------+---+-------+-----+\n|customer_id|product_id|quantity|date_of_purchase| id|   name|price|\n+-----------+----------+--------+----------------+---+-------+-----+\n|          2|         1|      12|      15-06-2023|  1|  fanta|   20|\n|       null|      null|    null|            null|  2|    dew|   22|\n|          9|         5|       6|      03-03-2023|  5| sprite|   40|\n|          2|         5|       3|      01-06-2023|  5| sprite|   40|\n|       null|      null|    null|            null|  7|redbull|  100|\n|         11|        12|      76|      12-03-2023| 12|  mazza|   45|\n|          5|        12|       5|      15-04-2023| 12|  mazza|   45|\n|          7|        22|       4|      03-02-2023| 22|   coke|   27|\n|          5|        22|       1|      22-03-2023| 22|   coke|   27|\n|          1|        22|      10|      01-06-2022| 22|   coke|   27|\n|       null|      null|    null|            null| 25|  limca|   21|\n|          1|        27|       5|      03-02-2023| 27|  pepsi|   14|\n|          1|        56|       2|      25-06-2023| 56|  sting|   10|\n+-----------+----------+--------+----------------+---+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "sales_df.join(product_df,sales_df['product_id']==product_df['id'],'right').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1081926a-64bc-4284-9c3c-54374c88bc0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n|customer_id|customer_name|address|date_of_joining|customer_id|product_id|quantity|date_of_purchase|\n+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n|          1|       manish|  patna|     30-05-2022|          1|        22|      10|      01-06-2022|\n|          1|       manish|  patna|     30-05-2022|          1|        27|       5|      03-02-2023|\n|          1|       manish|  patna|     30-05-2022|          1|        56|       2|      25-06-2023|\n|          2|       vikash|kolkata|     12-03-2023|          2|         5|       3|      01-06-2023|\n|          2|       vikash|kolkata|     12-03-2023|          2|         1|      12|      15-06-2023|\n|          3|       nikita|  delhi|     25-06-2023|       null|      null|    null|            null|\n|          4|        rahul| ranchi|     24-03-2023|       null|      null|    null|            null|\n|          5|       mahesh| jaipur|     22-03-2023|          5|        22|       1|      22-03-2023|\n|          5|       mahesh| jaipur|     22-03-2023|          5|        12|       5|      15-04-2023|\n|          6|     prantosh|kolkata|     18-10-2022|       null|      null|    null|            null|\n|          7|        raman|  patna|     30-12-2022|          7|        22|       4|      03-02-2023|\n|          8|      prakash| ranchi|     24-02-2023|       null|      null|    null|            null|\n|          9|       ragini|kolkata|     03-03-2023|          9|         5|       6|      03-03-2023|\n|         10|      raushan| jaipur|     05-02-2023|       null|      null|    null|            null|\n|       null|         null|   null|           null|         11|        12|      76|      12-03-2023|\n+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    " customer_df.join(sales_df,sales_df['customer_id']==customer_df['customer_id'],'outer').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e1ff856-943b-4fa2-a9a9-70d93e755d7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+\n|customer_id|customer_name|address|date_of_joining|\n+-----------+-------------+-------+---------------+\n|          1|       manish|  patna|     30-05-2022|\n|          2|       vikash|kolkata|     12-03-2023|\n|          5|       mahesh| jaipur|     22-03-2023|\n|          7|        raman|  patna|     30-12-2022|\n|          9|       ragini|kolkata|     03-03-2023|\n+-----------+-------------+-------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    " customer_df.join(sales_df,sales_df['customer_id']==customer_df['customer_id'],'left_semi').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21aac7b1-80d0-4723-9d0d-d04ec9db7db0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+\n|customer_id|customer_name|address|date_of_joining|\n+-----------+-------------+-------+---------------+\n|          3|       nikita|  delhi|     25-06-2023|\n|          4|        rahul| ranchi|     24-03-2023|\n|          6|     prantosh|kolkata|     18-10-2022|\n|          8|      prakash| ranchi|     24-02-2023|\n|         10|      raushan| jaipur|     05-02-2023|\n+-----------+-------------+-------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    " customer_df.join(sales_df,sales_df['customer_id']==customer_df['customer_id'],'left_anti').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d5377db-93f2-4da3-865a-e9b81e68cbfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n|customer_id|customer_name|address|date_of_joining|customer_id|product_id|quantity|date_of_purchase|\n+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n|          1|       manish|  patna|     30-05-2022|          1|        22|      10|      01-06-2022|\n|          1|       manish|  patna|     30-05-2022|          1|        27|       5|      03-02-2023|\n|          1|       manish|  patna|     30-05-2022|          2|         5|       3|      01-06-2023|\n|          1|       manish|  patna|     30-05-2022|          5|        22|       1|      22-03-2023|\n|          1|       manish|  patna|     30-05-2022|          7|        22|       4|      03-02-2023|\n|          1|       manish|  patna|     30-05-2022|          9|         5|       6|      03-03-2023|\n|          1|       manish|  patna|     30-05-2022|          2|         1|      12|      15-06-2023|\n|          1|       manish|  patna|     30-05-2022|          1|        56|       2|      25-06-2023|\n|          1|       manish|  patna|     30-05-2022|          5|        12|       5|      15-04-2023|\n|          1|       manish|  patna|     30-05-2022|         11|        12|      76|      12-03-2023|\n|          2|       vikash|kolkata|     12-03-2023|          1|        22|      10|      01-06-2022|\n|          2|       vikash|kolkata|     12-03-2023|          1|        27|       5|      03-02-2023|\n|          2|       vikash|kolkata|     12-03-2023|          2|         5|       3|      01-06-2023|\n|          2|       vikash|kolkata|     12-03-2023|          5|        22|       1|      22-03-2023|\n|          2|       vikash|kolkata|     12-03-2023|          7|        22|       4|      03-02-2023|\n|          2|       vikash|kolkata|     12-03-2023|          9|         5|       6|      03-03-2023|\n|          2|       vikash|kolkata|     12-03-2023|          2|         1|      12|      15-06-2023|\n|          2|       vikash|kolkata|     12-03-2023|          1|        56|       2|      25-06-2023|\n|          2|       vikash|kolkata|     12-03-2023|          5|        12|       5|      15-04-2023|\n|          2|       vikash|kolkata|     12-03-2023|         11|        12|      76|      12-03-2023|\n+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "customer_df.crossJoin(sales_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc64c9af-e996-437a-9d29-0395bb20cd83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[14]: 100"
     ]
    }
   ],
   "source": [
    "customer_df.crossJoin(sales_df).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cf8b4ad-a0f3-429f-8051-fa4f8449b6d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8b0cd15-6a52-4fb0-9ff4-a9cfa51e1dac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp_data = [(1,'manish',50000,'IT','m'),\n",
    "(2,'vikash',60000,'sales','m'),\n",
    "(3,'raushan',70000,'marketing','m'),\n",
    "(4,'mukesh',80000,'IT','m'),\n",
    "(5,'priti',90000,'sales','f'),\n",
    "(6,'nikita',45000,'marketing','f'),\n",
    "(7,'ragini',55000,'marketing','f'),\n",
    "(8,'rashi',100000,'IT','f'),\n",
    "(9,'aditya',65000,'IT','m'),\n",
    "(10,'rahul',50000,'marketing','m'),\n",
    "(11,'rakhi',50000,'IT','f'),\n",
    "(12,'akhilesh',90000,'sales','m')]\n",
    "\n",
    "emp_schema=['id','name','salary','dept','gender']\n",
    "\n",
    "emp_df=spark.createDataFrame(emp_data,emp_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cec5e036-b974-4f0f-ab93-2e78a6b74c27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n|     dept|sum(salary)|\n+---------+-----------+\n|       IT|     345000|\n|marketing|     220000|\n|    sales|     240000|\n+---------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# GROUP BY\n",
    "\n",
    "emp_df.groupBy('dept').agg(sum('salary')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a7c3550-d87b-4e70-84d4-0bf675e97a9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+---------+------+------------+\n| id|    name|salary|     dept|gender|Total_salary|\n+---+--------+------+---------+------+------------+\n|  1|  manish| 50000|       IT|     m|      345000|\n|  4|  mukesh| 80000|       IT|     m|      345000|\n|  8|   rashi|100000|       IT|     f|      345000|\n|  9|  aditya| 65000|       IT|     m|      345000|\n| 11|   rakhi| 50000|       IT|     f|      345000|\n|  3| raushan| 70000|marketing|     m|      220000|\n|  6|  nikita| 45000|marketing|     f|      220000|\n|  7|  ragini| 55000|marketing|     f|      220000|\n| 10|   rahul| 50000|marketing|     m|      220000|\n|  2|  vikash| 60000|    sales|     m|      240000|\n|  5|   priti| 90000|    sales|     f|      240000|\n| 12|akhilesh| 90000|    sales|     m|      240000|\n+---+--------+------+---------+------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "window = Window.partitionBy('dept')\n",
    "emp_df.withColumn('Total_salary',sum(col('salary')).over(window)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4102527a-7e07-4246-add0-0fd006d35eea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+---------+------+----------+----+---------+\n| id|    name|salary|     dept|gender|row_number|Rank|DenseRank|\n+---+--------+------+---------+------+----------+----+---------+\n|  1|  manish| 50000|       IT|     m|         1|   1|        1|\n| 11|   rakhi| 50000|       IT|     f|         2|   1|        1|\n|  9|  aditya| 65000|       IT|     m|         3|   3|        2|\n|  4|  mukesh| 80000|       IT|     m|         4|   4|        3|\n|  8|   rashi|100000|       IT|     f|         5|   5|        4|\n|  6|  nikita| 45000|marketing|     f|         1|   1|        1|\n| 10|   rahul| 50000|marketing|     m|         2|   2|        2|\n|  7|  ragini| 55000|marketing|     f|         3|   3|        3|\n|  3| raushan| 70000|marketing|     m|         4|   4|        4|\n|  2|  vikash| 60000|    sales|     m|         1|   1|        1|\n|  5|   priti| 90000|    sales|     f|         2|   2|        2|\n| 12|akhilesh| 90000|    sales|     m|         3|   2|        2|\n+---+--------+------+---------+------+----------+----+---------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "window = Window.partitionBy('dept').orderBy('salary')\n",
    "emp_df.withColumn('row_number',row_number().over(window))\\\n",
    "    .withColumn('Rank',rank().over(window))\\\n",
    "        .withColumn('DenseRank',dense_rank().over(window))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69b2c4d4-4103-45d8-831d-015728fee2ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+---------+------+----------+----+---------+\n| id|    name|salary|     dept|gender|row_number|Rank|DenseRank|\n+---+--------+------+---------+------+----------+----+---------+\n| 11|   rakhi| 50000|       IT|     f|         1|   1|        1|\n|  8|   rashi|100000|       IT|     f|         2|   2|        2|\n|  1|  manish| 50000|       IT|     m|         1|   1|        1|\n|  9|  aditya| 65000|       IT|     m|         2|   2|        2|\n|  4|  mukesh| 80000|       IT|     m|         3|   3|        3|\n|  6|  nikita| 45000|marketing|     f|         1|   1|        1|\n|  7|  ragini| 55000|marketing|     f|         2|   2|        2|\n| 10|   rahul| 50000|marketing|     m|         1|   1|        1|\n|  3| raushan| 70000|marketing|     m|         2|   2|        2|\n|  5|   priti| 90000|    sales|     f|         1|   1|        1|\n|  2|  vikash| 60000|    sales|     m|         1|   1|        1|\n| 12|akhilesh| 90000|    sales|     m|         2|   2|        2|\n+---+--------+------+---------+------+----------+----+---------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "window = Window.partitionBy('dept','gender').orderBy('salary')\n",
    "emp_df.withColumn('row_number',row_number().over(window))\\\n",
    "    .withColumn('Rank',rank().over(window))\\\n",
    "        .withColumn('DenseRank',dense_rank().over(window))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d80874c9-bfe4-495a-9987-97d221b28bc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+---------+------+----------+----+---------+\n| id|    name|salary|     dept|gender|row_number|Rank|DenseRank|\n+---+--------+------+---------+------+----------+----+---------+\n|  8|   rashi|100000|       IT|     f|         1|   1|        1|\n|  4|  mukesh| 80000|       IT|     m|         2|   2|        2|\n|  3| raushan| 70000|marketing|     m|         1|   1|        1|\n|  7|  ragini| 55000|marketing|     f|         2|   2|        2|\n|  5|   priti| 90000|    sales|     f|         1|   1|        1|\n| 12|akhilesh| 90000|    sales|     m|         2|   1|        1|\n|  2|  vikash| 60000|    sales|     m|         3|   3|        2|\n+---+--------+------+---------+------+----------+----+---------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "window = Window.partitionBy('dept').orderBy(desc('salary'))\n",
    "emp_df.withColumn('row_number',row_number().over(window))\\\n",
    "    .withColumn('Rank',rank().over(window))\\\n",
    "        .withColumn('DenseRank',dense_rank().over(window))\\\n",
    "            .filter(col('DenseRank')<=2)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6df4867-90c5-4b51-a3c8-726ef1a3af5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "product_data = [\n",
    "(1,\"iphone\",\"01-01-2023\",1500000),\n",
    "(2,\"samsung\",\"01-01-2023\",1100000),\n",
    "(3,\"oneplus\",\"01-01-2023\",1100000),\n",
    "(1,\"iphone\",\"01-02-2023\",1300000),\n",
    "(2,\"samsung\",\"01-02-2023\",1120000),\n",
    "(3,\"oneplus\",\"01-02-2023\",1120000),\n",
    "(1,\"iphone\",\"01-03-2023\",1600000),\n",
    "(2,\"samsung\",\"01-03-2023\",1080000),\n",
    "(3,\"oneplus\",\"01-03-2023\",1160000),\n",
    "(1,\"iphone\",\"01-04-2023\",1700000),\n",
    "(2,\"samsung\",\"01-04-2023\",1800000),\n",
    "(3,\"oneplus\",\"01-04-2023\",1170000),\n",
    "(1,\"iphone\",\"01-05-2023\",1200000),\n",
    "(2,\"samsung\",\"01-05-2023\",980000),\n",
    "(3,\"oneplus\",\"01-05-2023\",1175000),\n",
    "(1,\"iphone\",\"01-06-2023\",1100000),\n",
    "(2,\"samsung\",\"01-06-2023\",1100000),\n",
    "(3,\"oneplus\",\"01-06-2023\",1200000)\n",
    "]\n",
    "\n",
    "product_schema=['product_id','product_name','sales_date','sales']\n",
    "\n",
    "product_df=spark.createDataFrame(product_data,product_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "406964e9-990c-405d-a2bb-0bb3e524a70f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-------+---------------+\n|product_id|product_name|sales_date|  sales|prev_month_sale|\n+----------+------------+----------+-------+---------------+\n|         1|      iphone|01-01-2023|1500000|           null|\n|         1|      iphone|01-02-2023|1300000|        1500000|\n|         1|      iphone|01-03-2023|1600000|        1300000|\n|         1|      iphone|01-04-2023|1700000|        1600000|\n|         1|      iphone|01-05-2023|1200000|        1700000|\n|         1|      iphone|01-06-2023|1100000|        1200000|\n|         2|     samsung|01-01-2023|1100000|           null|\n|         2|     samsung|01-02-2023|1120000|        1100000|\n|         2|     samsung|01-03-2023|1080000|        1120000|\n|         2|     samsung|01-04-2023|1800000|        1080000|\n|         2|     samsung|01-05-2023| 980000|        1800000|\n|         2|     samsung|01-06-2023|1100000|         980000|\n|         3|     oneplus|01-01-2023|1100000|           null|\n|         3|     oneplus|01-02-2023|1120000|        1100000|\n|         3|     oneplus|01-03-2023|1160000|        1120000|\n|         3|     oneplus|01-04-2023|1170000|        1160000|\n|         3|     oneplus|01-05-2023|1175000|        1170000|\n|         3|     oneplus|01-06-2023|1200000|        1175000|\n+----------+------------+----------+-------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "window=Window.partitionBy('product_id').orderBy('sales_date')\n",
    "last_month_df= product_df.withColumn('prev_month_sale',lag(col('sales'),1).over(window))\n",
    "last_month_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f704370d-9a71-4c39-b477-86ac141a2ac4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-------+---------------+-------------+\n|product_id|product_name|sales_date|  sales|prev_month_sale|per_loss_gain|\n+----------+------------+----------+-------+---------------+-------------+\n|         1|      iphone|01-01-2023|1500000|           null|         null|\n|         1|      iphone|01-02-2023|1300000|        1500000|       -15.38|\n|         1|      iphone|01-03-2023|1600000|        1300000|        18.75|\n|         1|      iphone|01-04-2023|1700000|        1600000|         5.88|\n|         1|      iphone|01-05-2023|1200000|        1700000|       -41.67|\n|         1|      iphone|01-06-2023|1100000|        1200000|        -9.09|\n|         2|     samsung|01-01-2023|1100000|           null|         null|\n|         2|     samsung|01-02-2023|1120000|        1100000|         1.79|\n|         2|     samsung|01-03-2023|1080000|        1120000|         -3.7|\n|         2|     samsung|01-04-2023|1800000|        1080000|         40.0|\n|         2|     samsung|01-05-2023| 980000|        1800000|       -83.67|\n|         2|     samsung|01-06-2023|1100000|         980000|        10.91|\n|         3|     oneplus|01-01-2023|1100000|           null|         null|\n|         3|     oneplus|01-02-2023|1120000|        1100000|         1.79|\n|         3|     oneplus|01-03-2023|1160000|        1120000|         3.45|\n|         3|     oneplus|01-04-2023|1170000|        1160000|         0.85|\n|         3|     oneplus|01-05-2023|1175000|        1170000|         0.43|\n|         3|     oneplus|01-06-2023|1200000|        1175000|         2.08|\n+----------+------------+----------+-------+---------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "last_month_df.withColumn('per_loss_gain',round(((col(\"sales\")-col('prev_month_sale'))/col('sales'))*100,2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c743ec5-757b-4445-acfb-db0e935865b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Range and Row between / first and last"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "274660f4-b264-4e30-b9f6-d944515d85b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Difference in First and last Sale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2863c5e-7af1-49a4-91f8-1ac805c1ef29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ef8792e-93ea-4cbb-b6af-62890c6ff3c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-------+\n|product_id|product_name|sales_date|  sales|\n+----------+------------+----------+-------+\n|         2|     samsung|01-01-1995|  11000|\n|         1|      iphone|01-02-2023|1300000|\n|         2|     samsung|01-02-2023|1120000|\n|         3|     oneplus|01-02-2023|1120000|\n|         1|      iphone|01-03-2023|1600000|\n|         2|     samsung|01-03-2023|1080000|\n|         3|     oneplus|01-03-2023|1160000|\n|         1|      iphone|01-01-2006|  15000|\n|         1|      iphone|01-04-2023|1700000|\n|         2|     samsung|01-04-2023|1800000|\n|         3|     oneplus|01-04-2023|1170000|\n|         1|      iphone|01-05-2023|1200000|\n|         2|     samsung|01-05-2023| 980000|\n|         3|     oneplus|01-05-2023|1175000|\n|         1|      iphone|01-06-2023|1100000|\n|         3|     oneplus|01-01-2010|  23000|\n|         2|     samsung|01-06-2023|1100000|\n|         3|     oneplus|01-06-2023|1200000|\n+----------+------------+----------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "product_data = [\n",
    "(2,\"samsung\",\"01-01-1995\",11000),\n",
    "(1,\"iphone\",\"01-02-2023\",1300000),\n",
    "(2,\"samsung\",\"01-02-2023\",1120000),\n",
    "(3,\"oneplus\",\"01-02-2023\",1120000),\n",
    "(1,\"iphone\",\"01-03-2023\",1600000),\n",
    "(2,\"samsung\",\"01-03-2023\",1080000),\n",
    "(3,\"oneplus\",\"01-03-2023\",1160000),\n",
    "(1,\"iphone\",\"01-01-2006\",15000),\n",
    "(1,\"iphone\",\"01-04-2023\",1700000),\n",
    "(2,\"samsung\",\"01-04-2023\",1800000),\n",
    "(3,\"oneplus\",\"01-04-2023\",1170000),\n",
    "(1,\"iphone\",\"01-05-2023\",1200000),\n",
    "(2,\"samsung\",\"01-05-2023\",980000),\n",
    "(3,\"oneplus\",\"01-05-2023\",1175000),\n",
    "(1,\"iphone\",\"01-06-2023\",1100000),\n",
    "(3,\"oneplus\",\"01-01-2010\",23000),\n",
    "(2,\"samsung\",\"01-06-2023\",1100000),\n",
    "(3,\"oneplus\",\"01-06-2023\",1200000)\n",
    "]\n",
    "\n",
    "product_schema=[\"product_id\",\"product_name\",\"sales_date\",\"sales\"]\n",
    "\n",
    "product_df = spark.createDataFrame(data=product_data,schema=product_schema)\n",
    "\n",
    "product_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46b15411-a798-4b84-ad6d-a46eb30aa8a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window = Window.partitionBy('product_id').orderBy('sales_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "707f9297-fd28-410c-b50a-b1622498caec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-------+----------+---------+\n|product_id|product_name|sales_date|  sales|first_sale|last_sale|\n+----------+------------+----------+-------+----------+---------+\n|         1|      iphone|01-01-2006|  15000|     15000|    15000|\n|         1|      iphone|01-02-2023|1300000|     15000|  1300000|\n|         1|      iphone|01-03-2023|1600000|     15000|  1600000|\n|         1|      iphone|01-04-2023|1700000|     15000|  1700000|\n|         1|      iphone|01-05-2023|1200000|     15000|  1200000|\n|         1|      iphone|01-06-2023|1100000|     15000|  1100000|\n|         2|     samsung|01-01-1995|  11000|     11000|    11000|\n|         2|     samsung|01-02-2023|1120000|     11000|  1120000|\n|         2|     samsung|01-03-2023|1080000|     11000|  1080000|\n|         2|     samsung|01-04-2023|1800000|     11000|  1800000|\n|         2|     samsung|01-05-2023| 980000|     11000|   980000|\n|         2|     samsung|01-06-2023|1100000|     11000|  1100000|\n|         3|     oneplus|01-01-2010|  23000|     23000|    23000|\n|         3|     oneplus|01-02-2023|1120000|     23000|  1120000|\n|         3|     oneplus|01-03-2023|1160000|     23000|  1160000|\n|         3|     oneplus|01-04-2023|1170000|     23000|  1170000|\n|         3|     oneplus|01-05-2023|1175000|     23000|  1175000|\n|         3|     oneplus|01-06-2023|1200000|     23000|  1200000|\n+----------+------------+----------+-------+----------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "product_df.withColumn('first_sale',first('sales').over(window))\\\n",
    "    .withColumn('last_sale',last('sales').over(window))\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4df8fba4-6450-4950-98ea-9faa5a9510bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Window [product_id#2L, product_name#3, sales_date#4, sales#5L, first(sales#5L, false) windowspecdefinition(product_id#2L, sales_date#4 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS first_sale#105L, last(sales#5L, false) windowspecdefinition(product_id#2L, sales_date#4 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS last_sale#112L], [product_id#2L], [sales_date#4 ASC NULLS FIRST]\n   +- Sort [product_id#2L ASC NULLS FIRST, sales_date#4 ASC NULLS FIRST], false, 0\n      +- Exchange hashpartitioning(product_id#2L, 200), ENSURE_REQUIREMENTS, [plan_id=116]\n         +- Scan ExistingRDD[product_id#2L,product_name#3,sales_date#4,sales#5L]\n\n\n"
     ]
    }
   ],
   "source": [
    "product_df.withColumn('first_sale',first('sales').over(window))\\\n",
    "    .withColumn('last_sale',last('sales').over(window))\\\n",
    "        .explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63b64adf-9bc6-4919-928d-f1c5c1a342eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window = Window.partitionBy('product_id').orderBy('sales_date')\\\n",
    "    .rowsBetween(Window.unboundedPreceding,Window.unboundedFollowing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b9120fb-c470-4cd1-bb4b-fcd8ef96178f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-------+----------+---------+\n|product_id|product_name|sales_date|  sales|first_sale|last_sale|\n+----------+------------+----------+-------+----------+---------+\n|         1|      iphone|01-01-2006|  15000|     15000|  1100000|\n|         1|      iphone|01-02-2023|1300000|     15000|  1100000|\n|         1|      iphone|01-03-2023|1600000|     15000|  1100000|\n|         1|      iphone|01-04-2023|1700000|     15000|  1100000|\n|         1|      iphone|01-05-2023|1200000|     15000|  1100000|\n|         1|      iphone|01-06-2023|1100000|     15000|  1100000|\n|         2|     samsung|01-01-1995|  11000|     11000|  1100000|\n|         2|     samsung|01-02-2023|1120000|     11000|  1100000|\n|         2|     samsung|01-03-2023|1080000|     11000|  1100000|\n|         2|     samsung|01-04-2023|1800000|     11000|  1100000|\n|         2|     samsung|01-05-2023| 980000|     11000|  1100000|\n|         2|     samsung|01-06-2023|1100000|     11000|  1100000|\n|         3|     oneplus|01-01-2010|  23000|     23000|  1200000|\n|         3|     oneplus|01-02-2023|1120000|     23000|  1200000|\n|         3|     oneplus|01-03-2023|1160000|     23000|  1200000|\n|         3|     oneplus|01-04-2023|1170000|     23000|  1200000|\n|         3|     oneplus|01-05-2023|1175000|     23000|  1200000|\n|         3|     oneplus|01-06-2023|1200000|     23000|  1200000|\n+----------+------------+----------+-------+----------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "product_df.withColumn('first_sale',first('sales').over(window))\\\n",
    "    .withColumn('last_sale',last('sales').over(window))\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e4f0251-b080-45b1-a6e1-21eec10d36e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-------+\n|product_id|product_name|   diff|\n+----------+------------+-------+\n|         1|      iphone|1085000|\n|         2|     samsung|1089000|\n|         3|     oneplus|1177000|\n+----------+------------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "product_df.withColumn('first_sale',first('sales').over(window))\\\n",
    "    .withColumn('last_sale',last('sales').over(window))\\\n",
    "        .withColumn('diff',col('last_sale')-col('first_sale'))\\\n",
    "            .select('product_id','product_name','diff').distinct()\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b071ef08-28d5-44aa-85fd-687b4eee8dbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Employee not completing 8 hours in office"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc0fd39b-5b6b-4b49-ac82-52f817b00a6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+-----+\n| id|  name|      date| time|\n+---+------+----------+-----+\n|  1|manish|11-07-2023|10:20|\n|  1|manish|11-07-2023|11:20|\n|  2|rajesh|11-07-2023|11:20|\n|  1|manish|11-07-2023|11:50|\n|  2|rajesh|11-07-2023|13:20|\n|  1|manish|11-07-2023|19:20|\n|  2|rajesh|11-07-2023|17:20|\n|  1|manish|12-07-2023|10:32|\n|  1|manish|12-07-2023|12:20|\n|  3|vikash|12-07-2023|09:12|\n|  1|manish|12-07-2023|16:23|\n|  3|vikash|12-07-2023|18:08|\n+---+------+----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "emp_data = [(1,\"manish\",\"11-07-2023\",\"10:20\"),\n",
    "        (1,\"manish\",\"11-07-2023\",\"11:20\"),\n",
    "        (2,\"rajesh\",\"11-07-2023\",\"11:20\"),\n",
    "        (1,\"manish\",\"11-07-2023\",\"11:50\"),\n",
    "        (2,\"rajesh\",\"11-07-2023\",\"13:20\"),\n",
    "        (1,\"manish\",\"11-07-2023\",\"19:20\"),\n",
    "        (2,\"rajesh\",\"11-07-2023\",\"17:20\"),\n",
    "        (1,\"manish\",\"12-07-2023\",\"10:32\"),\n",
    "        (1,\"manish\",\"12-07-2023\",\"12:20\"),\n",
    "        (3,\"vikash\",\"12-07-2023\",\"09:12\"),\n",
    "        (1,\"manish\",\"12-07-2023\",\"16:23\"),\n",
    "        (3,\"vikash\",\"12-07-2023\",\"18:08\")]\n",
    "\n",
    "emp_schema = [\"id\", \"name\", \"date\", \"time\"]\n",
    "emp_df = spark.createDataFrame(data=emp_data, schema=emp_schema)\n",
    "\n",
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "253474b9-c192-4b21-9b8f-58fea7c80788",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp_df = emp_df.withColumn('timestamp',\n",
    "                           from_unixtime(unix_timestamp(expr('CONCAT(date,\" \",time)'),'dd-MM-yyyy HH:mm')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b67a6cc7-6048-48da-8ae6-957c8aa5a744",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+-----+-------------------+\n| id|  name|      date| time|          timestamp|\n+---+------+----------+-----+-------------------+\n|  1|manish|11-07-2023|10:20|2023-07-11 10:20:00|\n|  1|manish|11-07-2023|11:20|2023-07-11 11:20:00|\n|  2|rajesh|11-07-2023|11:20|2023-07-11 11:20:00|\n|  1|manish|11-07-2023|11:50|2023-07-11 11:50:00|\n|  2|rajesh|11-07-2023|13:20|2023-07-11 13:20:00|\n|  1|manish|11-07-2023|19:20|2023-07-11 19:20:00|\n|  2|rajesh|11-07-2023|17:20|2023-07-11 17:20:00|\n|  1|manish|12-07-2023|10:32|2023-07-12 10:32:00|\n|  1|manish|12-07-2023|12:20|2023-07-12 12:20:00|\n|  3|vikash|12-07-2023|09:12|2023-07-12 09:12:00|\n|  1|manish|12-07-2023|16:23|2023-07-12 16:23:00|\n|  3|vikash|12-07-2023|18:08|2023-07-12 18:08:00|\n+---+------+----------+-----+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31da6bea-06b9-42ae-a219-26d7e6b17564",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window = Window.partitionBy('id','date').orderBy('date')\\\n",
    "    .rowsBetween(Window.unboundedPreceding,Window.unboundedFollowing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5ac79af-c85f-403e-856b-60962c5fc66d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+-----+-------------------+-------------------+-------------------+\n| id|  name|      date| time|          timestamp|              login|             logout|\n+---+------+----------+-----+-------------------+-------------------+-------------------+\n|  1|manish|11-07-2023|10:20|2023-07-11 10:20:00|2023-07-11 10:20:00|2023-07-11 19:20:00|\n|  1|manish|11-07-2023|11:20|2023-07-11 11:20:00|2023-07-11 10:20:00|2023-07-11 19:20:00|\n|  1|manish|11-07-2023|11:50|2023-07-11 11:50:00|2023-07-11 10:20:00|2023-07-11 19:20:00|\n|  1|manish|11-07-2023|19:20|2023-07-11 19:20:00|2023-07-11 10:20:00|2023-07-11 19:20:00|\n|  1|manish|12-07-2023|10:32|2023-07-12 10:32:00|2023-07-12 10:32:00|2023-07-12 16:23:00|\n|  1|manish|12-07-2023|12:20|2023-07-12 12:20:00|2023-07-12 10:32:00|2023-07-12 16:23:00|\n|  1|manish|12-07-2023|16:23|2023-07-12 16:23:00|2023-07-12 10:32:00|2023-07-12 16:23:00|\n|  2|rajesh|11-07-2023|11:20|2023-07-11 11:20:00|2023-07-11 11:20:00|2023-07-11 17:20:00|\n|  2|rajesh|11-07-2023|13:20|2023-07-11 13:20:00|2023-07-11 11:20:00|2023-07-11 17:20:00|\n|  2|rajesh|11-07-2023|17:20|2023-07-11 17:20:00|2023-07-11 11:20:00|2023-07-11 17:20:00|\n|  3|vikash|12-07-2023|09:12|2023-07-12 09:12:00|2023-07-12 09:12:00|2023-07-12 18:08:00|\n|  3|vikash|12-07-2023|18:08|2023-07-12 18:08:00|2023-07-12 09:12:00|2023-07-12 18:08:00|\n+---+------+----------+-----+-------------------+-------------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.withColumn('login',first('timestamp').over(window))\\\n",
    "    .withColumn('logout',last('timestamp').over(window)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "000a49c8-b9cc-4315-a821-adcaebd9c75a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- date: string (nullable = true)\n |-- time: string (nullable = true)\n |-- timestamp: string (nullable = true)\n |-- login: string (nullable = true)\n |-- logout: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.withColumn('login',first('timestamp').over(window))\\\n",
    "    .withColumn('logout',last('timestamp').over(window)).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8afce61-f870-43f9-a267-b8e8f1e02325",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+-----+-------------------+-------------------+-------------------+-----------------------------------+\n|id |name  |date      |time |timestamp          |login              |logout             |total_time                         |\n+---+------+----------+-----+-------------------+-------------------+-------------------+-----------------------------------+\n|1  |manish|11-07-2023|10:20|2023-07-11 10:20:00|2023-07-11 10:20:00|2023-07-11 19:20:00|INTERVAL '0 09:00:00' DAY TO SECOND|\n|1  |manish|11-07-2023|11:20|2023-07-11 11:20:00|2023-07-11 10:20:00|2023-07-11 19:20:00|INTERVAL '0 09:00:00' DAY TO SECOND|\n|1  |manish|11-07-2023|11:50|2023-07-11 11:50:00|2023-07-11 10:20:00|2023-07-11 19:20:00|INTERVAL '0 09:00:00' DAY TO SECOND|\n|1  |manish|11-07-2023|19:20|2023-07-11 19:20:00|2023-07-11 10:20:00|2023-07-11 19:20:00|INTERVAL '0 09:00:00' DAY TO SECOND|\n|1  |manish|12-07-2023|10:32|2023-07-12 10:32:00|2023-07-12 10:32:00|2023-07-12 16:23:00|INTERVAL '0 05:51:00' DAY TO SECOND|\n|1  |manish|12-07-2023|12:20|2023-07-12 12:20:00|2023-07-12 10:32:00|2023-07-12 16:23:00|INTERVAL '0 05:51:00' DAY TO SECOND|\n|1  |manish|12-07-2023|16:23|2023-07-12 16:23:00|2023-07-12 10:32:00|2023-07-12 16:23:00|INTERVAL '0 05:51:00' DAY TO SECOND|\n|2  |rajesh|11-07-2023|11:20|2023-07-11 11:20:00|2023-07-11 11:20:00|2023-07-11 17:20:00|INTERVAL '0 06:00:00' DAY TO SECOND|\n|2  |rajesh|11-07-2023|13:20|2023-07-11 13:20:00|2023-07-11 11:20:00|2023-07-11 17:20:00|INTERVAL '0 06:00:00' DAY TO SECOND|\n|2  |rajesh|11-07-2023|17:20|2023-07-11 17:20:00|2023-07-11 11:20:00|2023-07-11 17:20:00|INTERVAL '0 06:00:00' DAY TO SECOND|\n|3  |vikash|12-07-2023|09:12|2023-07-12 09:12:00|2023-07-12 09:12:00|2023-07-12 18:08:00|INTERVAL '0 08:56:00' DAY TO SECOND|\n|3  |vikash|12-07-2023|18:08|2023-07-12 18:08:00|2023-07-12 09:12:00|2023-07-12 18:08:00|INTERVAL '0 08:56:00' DAY TO SECOND|\n+---+------+----------+-----+-------------------+-------------------+-------------------+-----------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.withColumn('login',first('timestamp').over(window))\\\n",
    "    .withColumn('logout',last('timestamp').over(window))\\\n",
    "        .withColumn('login',to_timestamp('login','yyyy-MM-dd HH:mm:ss'))\\\n",
    "            .withColumn('logout',to_timestamp('logout','yyyy-MM-dd HH:mm:ss'))\\\n",
    "                .withColumn('total_time',col('logout')-col('login')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00da871b-7b33-4fbc-b644-2fbfb7699841",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+--------------------+\n| id|  name|      date|          total_time|\n+---+------+----------+--------------------+\n|  1|manish|11-07-2023|INTERVAL '0 09:00...|\n|  1|manish|12-07-2023|INTERVAL '0 05:51...|\n|  2|rajesh|11-07-2023|INTERVAL '0 06:00...|\n|  3|vikash|12-07-2023|INTERVAL '0 08:56...|\n+---+------+----------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.withColumn('login',first('timestamp').over(window))\\\n",
    "    .withColumn('logout',last('timestamp').over(window))\\\n",
    "        .withColumn('login',to_timestamp('login','yyyy-MM-dd HH:mm:ss'))\\\n",
    "            .withColumn('logout',to_timestamp('logout','yyyy-MM-dd HH:mm:ss'))\\\n",
    "                .withColumn('total_time',col('logout')-col('login'))\\\n",
    "                    .select('id','name',\"date\",'total_time').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dd90e0f-bc43-4aad-9ab3-20dadcda6156",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp_df=emp_df.withColumn('login',first('timestamp').over(window))\\\n",
    "    .withColumn('logout',last('timestamp').over(window))\\\n",
    "        .withColumn('login',to_timestamp('login','yyyy-MM-dd HH:mm:ss'))\\\n",
    "            .withColumn('logout',to_timestamp('logout','yyyy-MM-dd HH:mm:ss'))\\\n",
    "                .withColumn('total_time',col('logout')-col('login'))\\\n",
    "                .select('id','name',\"date\",'total_time').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2790b1d9-308f-4e00-825f-8dca99532493",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp_df.createOrReplaceTempView('tbl')\n",
    "#emp_df = emp_df.withColumn('in_hrs',expr(\"extract(epoch from total_time) / 3600\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e5502a5-d369-49db-b7ad-0eb4a761662d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp_df=spark.sql(\"\"\"\n",
    "          select id , name, date ,extract(hour from total_time) as hours from tbl \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27f04792-5351-407a-9e33-36b3c9edd717",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- date: string (nullable = true)\n |-- hours: byte (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0ad4ebf-e697-4b34-9b50-6b9ed6cea0a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp_df = emp_df.withColumn('hours',col('hours').cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22096109-d0ba-40be-b2c8-017e8bcadc2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+-----+\n| id|  name|      date|hours|\n+---+------+----------+-----+\n|  1|manish|12-07-2023|    5|\n|  2|rajesh|11-07-2023|    6|\n+---+------+----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.filter(col('hours')<8).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88174933-d18a-4102-bcfb-cc015a2b2b04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Running sales/cumulative sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "142b857e-b72e-47fd-8a1b-15227082d1d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "product_data = [\n",
    "(1,\"iphone\",\"01-01-2023\",1500000),\n",
    "(2,\"samsung\",\"01-01-2023\",1100000),\n",
    "(3,\"oneplus\",\"01-01-2023\",1100000),\n",
    "(1,\"iphone\",\"01-02-2023\",1300000),\n",
    "(2,\"samsung\",\"01-02-2023\",1120000),\n",
    "(3,\"oneplus\",\"01-02-2023\",1120000),\n",
    "(1,\"iphone\",\"01-03-2023\",1600000),\n",
    "(2,\"samsung\",\"01-03-2023\",1080000),\n",
    "(3,\"oneplus\",\"01-03-2023\",1160000),\n",
    "(1,\"iphone\",\"01-04-2023\",1700000),\n",
    "(2,\"samsung\",\"01-04-2023\",1800000),\n",
    "(3,\"oneplus\",\"01-04-2023\",1170000),\n",
    "(1,\"iphone\",\"01-05-2023\",1200000),\n",
    "(2,\"samsung\",\"01-05-2023\",980000),\n",
    "(3,\"oneplus\",\"01-05-2023\",1175000),\n",
    "(1,\"iphone\",\"01-06-2023\",1100000),\n",
    "(2,\"samsung\",\"01-06-2023\",1100000),\n",
    "(3,\"oneplus\",\"01-06-2023\",1200000)\n",
    "]\n",
    "\n",
    "product_schema=[\"product_id\",\"product_name\",\"sales_date\",\"sales\"]\n",
    "\n",
    "product_df = spark.createDataFrame(data=product_data,schema=product_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c489db4-67ec-4e0a-ada7-ab4029f63629",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-------+\n|product_id|product_name|sales_date|  sales|\n+----------+------------+----------+-------+\n|         1|      iphone|01-01-2023|1500000|\n|         2|     samsung|01-01-2023|1100000|\n|         3|     oneplus|01-01-2023|1100000|\n|         1|      iphone|01-02-2023|1300000|\n|         2|     samsung|01-02-2023|1120000|\n|         3|     oneplus|01-02-2023|1120000|\n|         1|      iphone|01-03-2023|1600000|\n|         2|     samsung|01-03-2023|1080000|\n|         3|     oneplus|01-03-2023|1160000|\n|         1|      iphone|01-04-2023|1700000|\n|         2|     samsung|01-04-2023|1800000|\n|         3|     oneplus|01-04-2023|1170000|\n|         1|      iphone|01-05-2023|1200000|\n|         2|     samsung|01-05-2023| 980000|\n|         3|     oneplus|01-05-2023|1175000|\n|         1|      iphone|01-06-2023|1100000|\n|         2|     samsung|01-06-2023|1100000|\n|         3|     oneplus|01-06-2023|1200000|\n+----------+------------+----------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "product_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f4fba15-404b-4cb3-9405-88c09daa64f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window= Window.partitionBy('product_id').orderBy('sales_date').rowsBetween(-2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74476a25-348c-4dd8-b264-8a3027488aba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-------+-----------+\n|product_id|product_name|sales_date|  sales|running_sum|\n+----------+------------+----------+-------+-----------+\n|         1|      iphone|01-01-2023|1500000|    1500000|\n|         1|      iphone|01-02-2023|1300000|    2800000|\n|         1|      iphone|01-03-2023|1600000|    4400000|\n|         1|      iphone|01-04-2023|1700000|    4600000|\n|         1|      iphone|01-05-2023|1200000|    4500000|\n|         1|      iphone|01-06-2023|1100000|    4000000|\n|         2|     samsung|01-01-2023|1100000|    1100000|\n|         2|     samsung|01-02-2023|1120000|    2220000|\n|         2|     samsung|01-03-2023|1080000|    3300000|\n|         2|     samsung|01-04-2023|1800000|    4000000|\n|         2|     samsung|01-05-2023| 980000|    3860000|\n|         2|     samsung|01-06-2023|1100000|    3880000|\n|         3|     oneplus|01-01-2023|1100000|    1100000|\n|         3|     oneplus|01-02-2023|1120000|    2220000|\n|         3|     oneplus|01-03-2023|1160000|    3380000|\n|         3|     oneplus|01-04-2023|1170000|    3450000|\n|         3|     oneplus|01-05-2023|1175000|    3505000|\n|         3|     oneplus|01-06-2023|1200000|    3545000|\n+----------+------------+----------+-------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "product_df = product_df.withColumn('running_sum',sum('sales').over(window))\n",
    "product_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "950e3985-f7ec-4cfb-9064-07f132a9eb56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-------+-----------+----------+\n|product_id|product_name|sales_date|  sales|running_sum|row_number|\n+----------+------------+----------+-------+-----------+----------+\n|         1|      iphone|01-01-2023|1500000|    1500000|         1|\n|         1|      iphone|01-02-2023|1300000|    2800000|         2|\n|         1|      iphone|01-03-2023|1600000|    4400000|         3|\n|         1|      iphone|01-04-2023|1700000|    4600000|         4|\n|         1|      iphone|01-05-2023|1200000|    4500000|         5|\n|         1|      iphone|01-06-2023|1100000|    4000000|         6|\n|         2|     samsung|01-01-2023|1100000|    1100000|         1|\n|         2|     samsung|01-02-2023|1120000|    2220000|         2|\n|         2|     samsung|01-03-2023|1080000|    3300000|         3|\n|         2|     samsung|01-04-2023|1800000|    4000000|         4|\n|         2|     samsung|01-05-2023| 980000|    3860000|         5|\n|         2|     samsung|01-06-2023|1100000|    3880000|         6|\n|         3|     oneplus|01-01-2023|1100000|    1100000|         1|\n|         3|     oneplus|01-02-2023|1120000|    2220000|         2|\n|         3|     oneplus|01-03-2023|1160000|    3380000|         3|\n|         3|     oneplus|01-04-2023|1170000|    3450000|         4|\n|         3|     oneplus|01-05-2023|1175000|    3505000|         5|\n|         3|     oneplus|01-06-2023|1200000|    3545000|         6|\n+----------+------------+----------+-------+-----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "window2= Window.partitionBy('product_id').orderBy('sales_date')\n",
    "product_df = product_df.withColumn('row_number',row_number().over(window2))\n",
    "product_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3946486-2475-4a2b-b365-e400cd43560a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "product_df = product_df.filter(col('row_number')>2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d3adb19-0673-4186-808a-7b61b6d325cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = product_df.withColumn('Avg_sale',round(col('running_sum')/3,2)).select('product_id','product_name','sales_date','Avg_sale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22bb4b6b-87d1-4117-ac10-307aa039fd32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+----------+\n|product_id|product_name|sales_date|  Avg_sale|\n+----------+------------+----------+----------+\n|         1|      iphone|01-03-2023|1466666.67|\n|         1|      iphone|01-04-2023|1533333.33|\n|         1|      iphone|01-05-2023| 1500000.0|\n|         1|      iphone|01-06-2023|1333333.33|\n|         2|     samsung|01-03-2023| 1100000.0|\n|         2|     samsung|01-04-2023|1333333.33|\n|         2|     samsung|01-05-2023|1286666.67|\n|         2|     samsung|01-06-2023|1293333.33|\n|         3|     oneplus|01-03-2023|1126666.67|\n|         3|     oneplus|01-04-2023| 1150000.0|\n|         3|     oneplus|01-05-2023|1168333.33|\n|         3|     oneplus|01-06-2023|1181666.67|\n+----------+------------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "703f7728-8e71-4bba-9241-c7270f4c66a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Flatten json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b8fdafe-f861-4f5b-afff-a467a8e610e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "restaurant_json_data = spark.read.format('json')\\\n",
    "  .option('multiline','true')\\\n",
    "    .option('inferschema','true')\\\n",
    "      .load('/FileStore/tables/resturant_json_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba69a2c6-c853-4e19-9bef-110ac26d8724",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[0m\u001B[01;34mazure\u001B[0m/  \u001B[01;34meventlogs\u001B[0m/  \u001B[01;32mhadoop_accessed_config.lst\u001B[0m*  \u001B[01;32mpreload_class.lst\u001B[0m*\r\n\u001B[01;34mconf\u001B[0m/   \u001B[01;34mganglia\u001B[0m/    \u001B[01;34mlogs\u001B[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14622874-ceb9-4c96-b88d-2bb5764b3a67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/tables/2010_summary.csv</td><td>2010_summary.csv</td><td>7121</td><td>1734530986000</td></tr><tr><td>dbfs:/FileStore/tables/Multi_line_correct.json</td><td>Multi_line_correct.json</td><td>310</td><td>1734629277000</td></tr><tr><td>dbfs:/FileStore/tables/Multi_line_incorrect.json</td><td>Multi_line_incorrect.json</td><td>304</td><td>1734629277000</td></tr><tr><td>dbfs:/FileStore/tables/StudentData.csv</td><td>StudentData.csv</td><td>75701</td><td>1656587787000</td></tr><tr><td>dbfs:/FileStore/tables/bad_records/</td><td>bad_records/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/bucket_by_id/</td><td>bucket_by_id/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/corrupted_json.json</td><td>corrupted_json.json</td><td>218</td><td>1734629275000</td></tr><tr><td>dbfs:/FileStore/tables/csv_write/</td><td>csv_write/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/csv_write_repartition/</td><td>csv_write_repartition/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/distinct.txt</td><td>distinct.txt</td><td>900</td><td>1656580239000</td></tr><tr><td>dbfs:/FileStore/tables/employee.csv</td><td>employee.csv</td><td>230</td><td>1734622538000</td></tr><tr><td>dbfs:/FileStore/tables/employees.csv</td><td>employees.csv</td><td>444</td><td>1734883812000</td></tr><tr><td>dbfs:/FileStore/tables/filter.txt</td><td>filter.txt</td><td>45</td><td>1656579325000</td></tr><tr><td>dbfs:/FileStore/tables/groupreduce.txt</td><td>groupreduce.txt</td><td>3226646</td><td>1656582669000</td></tr><tr><td>dbfs:/FileStore/tables/line_delimited_json.json</td><td>line_delimited_json.json</td><td>219</td><td>1734629276000</td></tr><tr><td>dbfs:/FileStore/tables/numbers.txt</td><td>numbers.txt</td><td>29</td><td>1656409579000</td></tr><tr><td>dbfs:/FileStore/tables/part_r_00000_1a9822ba_b8fb_4d8e_844a_ea30d0801b9e_gz.parquet</td><td>part_r_00000_1a9822ba_b8fb_4d8e_844a_ea30d0801b9e_gz.parquet</td><td>3921</td><td>1734798160000</td></tr><tr><td>dbfs:/FileStore/tables/partitionBy_address/</td><td>partitionBy_address/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/partitionBy_address_gender/</td><td>partitionBy_address_gender/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/partition_by_address/</td><td>partition_by_address/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/person.csv</td><td>person.csv</td><td>741</td><td>1734881047000</td></tr><tr><td>dbfs:/FileStore/tables/resturant_json_data.json</td><td>resturant_json_data.json</td><td>669503</td><td>1735478237000</td></tr><tr><td>dbfs:/FileStore/tables/single_file_json_with_extra_fields.json</td><td>single_file_json_with_extra_fields.json</td><td>234</td><td>1734629278000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/tables/2010_summary.csv",
         "2010_summary.csv",
         7121,
         1734530986000
        ],
        [
         "dbfs:/FileStore/tables/Multi_line_correct.json",
         "Multi_line_correct.json",
         310,
         1734629277000
        ],
        [
         "dbfs:/FileStore/tables/Multi_line_incorrect.json",
         "Multi_line_incorrect.json",
         304,
         1734629277000
        ],
        [
         "dbfs:/FileStore/tables/StudentData.csv",
         "StudentData.csv",
         75701,
         1656587787000
        ],
        [
         "dbfs:/FileStore/tables/bad_records/",
         "bad_records/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/bucket_by_id/",
         "bucket_by_id/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/corrupted_json.json",
         "corrupted_json.json",
         218,
         1734629275000
        ],
        [
         "dbfs:/FileStore/tables/csv_write/",
         "csv_write/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/csv_write_repartition/",
         "csv_write_repartition/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/distinct.txt",
         "distinct.txt",
         900,
         1656580239000
        ],
        [
         "dbfs:/FileStore/tables/employee.csv",
         "employee.csv",
         230,
         1734622538000
        ],
        [
         "dbfs:/FileStore/tables/employees.csv",
         "employees.csv",
         444,
         1734883812000
        ],
        [
         "dbfs:/FileStore/tables/filter.txt",
         "filter.txt",
         45,
         1656579325000
        ],
        [
         "dbfs:/FileStore/tables/groupreduce.txt",
         "groupreduce.txt",
         3226646,
         1656582669000
        ],
        [
         "dbfs:/FileStore/tables/line_delimited_json.json",
         "line_delimited_json.json",
         219,
         1734629276000
        ],
        [
         "dbfs:/FileStore/tables/numbers.txt",
         "numbers.txt",
         29,
         1656409579000
        ],
        [
         "dbfs:/FileStore/tables/part_r_00000_1a9822ba_b8fb_4d8e_844a_ea30d0801b9e_gz.parquet",
         "part_r_00000_1a9822ba_b8fb_4d8e_844a_ea30d0801b9e_gz.parquet",
         3921,
         1734798160000
        ],
        [
         "dbfs:/FileStore/tables/partitionBy_address/",
         "partitionBy_address/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/partitionBy_address_gender/",
         "partitionBy_address_gender/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/partition_by_address/",
         "partition_by_address/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/person.csv",
         "person.csv",
         741,
         1734881047000
        ],
        [
         "dbfs:/FileStore/tables/resturant_json_data.json",
         "resturant_json_data.json",
         669503,
         1735478237000
        ],
        [
         "dbfs:/FileStore/tables/single_file_json_with_extra_fields.json",
         "single_file_json_with_extra_fields.json",
         234,
         1734629278000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs\n",
    "ls /FileStore/tables/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67131ccf-6364-4944-9c26-c08b6137ab03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------------------+-------------+-------------+-------------+------+\n|code|message|         restaurants|results_found|results_shown|results_start|status|\n+----+-------+--------------------+-------------+-------------+-------------+------+\n|null|   null|                  []|            0|            0|            1|  null|\n|null|   null|[{{{17066603}, b9...|         6835|           20|            1|  null|\n|null|   null|                  []|            0|            0|            1|  null|\n|null|   null|                  []|            0|            0|            1|  null|\n|null|   null|[{{{17093124}, b9...|         8680|           20|            1|  null|\n|null|   null|                  []|            0|            0|            1|  null|\n|null|   null|                  []|            0|            0|            1|  null|\n|null|   null|[{{{17580142}, b9...|          943|           20|            1|  null|\n|null|   null|                  []|            0|            0|            1|  null|\n|null|   null|                  []|            0|            0|            1|  null|\n|null|   null|[{{{17284158}, b9...|          257|           20|            1|  null|\n|null|   null|                  []|            0|            0|            1|  null|\n|null|   null|                  []|            0|            0|            1|  null|\n|null|   null|[{{{17678233}, b9...|          358|           20|            1|  null|\n|null|   null|                  []|            0|            0|            1|  null|\n|null|   null|                  []|            0|            0|            1|  null|\n|null|   null|[{{{17375047}, b9...|          641|           20|            1|  null|\n|null|   null|                  []|            0|            0|            1|  null|\n|null|   null|                  []|            0|            0|            1|  null|\n|null|   null|[{{{17616590}, b9...|         1613|           20|            1|  null|\n+----+-------+--------------------+-------------+-------------+-------------+------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "restaurant_json_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d32970fd-eb36-4a57-a4dd-cbeb1c559a79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- code: long (nullable = true)\n |-- message: string (nullable = true)\n |-- restaurants: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- restaurant: struct (nullable = true)\n |    |    |    |-- R: struct (nullable = true)\n |    |    |    |    |-- res_id: long (nullable = true)\n |    |    |    |-- apikey: string (nullable = true)\n |    |    |    |-- average_cost_for_two: long (nullable = true)\n |    |    |    |-- cuisines: string (nullable = true)\n |    |    |    |-- currency: string (nullable = true)\n |    |    |    |-- deeplink: string (nullable = true)\n |    |    |    |-- establishment_types: array (nullable = true)\n |    |    |    |    |-- element: string (containsNull = true)\n |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |-- featured_image: string (nullable = true)\n |    |    |    |-- has_online_delivery: long (nullable = true)\n |    |    |    |-- has_table_booking: long (nullable = true)\n |    |    |    |-- id: string (nullable = true)\n |    |    |    |-- is_delivering_now: long (nullable = true)\n |    |    |    |-- location: struct (nullable = true)\n |    |    |    |    |-- address: string (nullable = true)\n |    |    |    |    |-- city: string (nullable = true)\n |    |    |    |    |-- city_id: long (nullable = true)\n |    |    |    |    |-- country_id: long (nullable = true)\n |    |    |    |    |-- latitude: string (nullable = true)\n |    |    |    |    |-- locality: string (nullable = true)\n |    |    |    |    |-- locality_verbose: string (nullable = true)\n |    |    |    |    |-- longitude: string (nullable = true)\n |    |    |    |    |-- zipcode: string (nullable = true)\n |    |    |    |-- menu_url: string (nullable = true)\n |    |    |    |-- name: string (nullable = true)\n |    |    |    |-- offers: array (nullable = true)\n |    |    |    |    |-- element: string (containsNull = true)\n |    |    |    |-- photos_url: string (nullable = true)\n |    |    |    |-- price_range: long (nullable = true)\n |    |    |    |-- switch_to_order_menu: long (nullable = true)\n |    |    |    |-- thumb: string (nullable = true)\n |    |    |    |-- url: string (nullable = true)\n |    |    |    |-- user_rating: struct (nullable = true)\n |    |    |    |    |-- aggregate_rating: string (nullable = true)\n |    |    |    |    |-- rating_color: string (nullable = true)\n |    |    |    |    |-- rating_text: string (nullable = true)\n |    |    |    |    |-- votes: string (nullable = true)\n |-- results_found: long (nullable = true)\n |-- results_shown: long (nullable = true)\n |-- results_start: string (nullable = true)\n |-- status: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "restaurant_json_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f164589e-ef2c-4d07-b443-26aa9fb56181",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce66360c-0fd6-4939-ad67-46245703e4f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- code: long (nullable = true)\n |-- message: string (nullable = true)\n |-- results_found: long (nullable = true)\n |-- results_shown: long (nullable = true)\n |-- results_start: string (nullable = true)\n |-- status: string (nullable = true)\n |-- new_restaurants: struct (nullable = true)\n |    |-- restaurant: struct (nullable = true)\n |    |    |-- R: struct (nullable = true)\n |    |    |    |-- res_id: long (nullable = true)\n |    |    |-- apikey: string (nullable = true)\n |    |    |-- average_cost_for_two: long (nullable = true)\n |    |    |-- cuisines: string (nullable = true)\n |    |    |-- currency: string (nullable = true)\n |    |    |-- deeplink: string (nullable = true)\n |    |    |-- establishment_types: array (nullable = true)\n |    |    |    |-- element: string (containsNull = true)\n |    |    |-- events_url: string (nullable = true)\n |    |    |-- featured_image: string (nullable = true)\n |    |    |-- has_online_delivery: long (nullable = true)\n |    |    |-- has_table_booking: long (nullable = true)\n |    |    |-- id: string (nullable = true)\n |    |    |-- is_delivering_now: long (nullable = true)\n |    |    |-- location: struct (nullable = true)\n |    |    |    |-- address: string (nullable = true)\n |    |    |    |-- city: string (nullable = true)\n |    |    |    |-- city_id: long (nullable = true)\n |    |    |    |-- country_id: long (nullable = true)\n |    |    |    |-- latitude: string (nullable = true)\n |    |    |    |-- locality: string (nullable = true)\n |    |    |    |-- locality_verbose: string (nullable = true)\n |    |    |    |-- longitude: string (nullable = true)\n |    |    |    |-- zipcode: string (nullable = true)\n |    |    |-- menu_url: string (nullable = true)\n |    |    |-- name: string (nullable = true)\n |    |    |-- offers: array (nullable = true)\n |    |    |    |-- element: string (containsNull = true)\n |    |    |-- photos_url: string (nullable = true)\n |    |    |-- price_range: long (nullable = true)\n |    |    |-- switch_to_order_menu: long (nullable = true)\n |    |    |-- thumb: string (nullable = true)\n |    |    |-- url: string (nullable = true)\n |    |    |-- user_rating: struct (nullable = true)\n |    |    |    |-- aggregate_rating: string (nullable = true)\n |    |    |    |-- rating_color: string (nullable = true)\n |    |    |    |-- rating_text: string (nullable = true)\n |    |    |    |-- votes: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "restaurant_json_data.select('*',explode('restaurants').alias('new_restaurants')).drop('restaurants').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0b9f06f-e854-4cd7-953e-df56444799d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n|                name|\n+--------------------+\n|            The Coop|\n|Maggiano's Little...|\n|Tako Cheena by Po...|\n|Bosphorous Turkis...|\n|Bahama Breeze Isl...|\n|Hawkers Asian Str...|\n|Seasons 52 Fresh ...|\n|Raglan Road Irish...|\n|           Hillstone|\n|Hollerbach's Will...|\n|     Texas de Brazil|\n|    The Ravenous Pig|\n|    Earl of Sandwich|\n|    Café Tu Tu Tango|\n|Tibby's New Orlea...|\n|Cevíche Tapas Bar...|\n| Ethos Vegan Kitchen|\n|Pom Pom's Teahous...|\n|     Yellow Dog Eats|\n|              'Ohana|\n+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "restaurant_json_data.select('*',explode('restaurants').alias('new_restaurants')).drop('restaurants')\\\n",
    ".select('new_restaurants.restaurant.name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d8b5a94-33fd-40e8-b335-b5e271230a4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "SCD-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17f0b784-16e2-4c64-9ac3-ebed578a9111",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_dim_data = [\n",
    "\n",
    "(1,'manish','arwal','india','N','2022-09-15','2022-09-25'),\n",
    "(2,'vikash','patna','india','Y','2023-08-12',None),\n",
    "(3,'nikita','delhi','india','Y','2023-09-10',None),\n",
    "(4,'rakesh','jaipur','india','Y','2023-06-10',None),\n",
    "(5,'ayush','NY','USA','Y','2023-06-10',None),\n",
    "(1,'manish','gurgaon','india','Y','2022-09-25',None),\n",
    "]\n",
    "\n",
    "customer_schema= ['id','name','city','country','active','effective_start_date','effective_end_date']\n",
    "\n",
    "customer_dim_df = spark.createDataFrame(data= customer_dim_data,schema=customer_schema)\n",
    "\n",
    "sales_data = [\n",
    "\n",
    "(1,1,'manish','2023-01-16','gurgaon','india',380),\n",
    "(77,1,'manish','2023-03-11','bangalore','india',300),\n",
    "(12,3,'nikita','2023-09-20','delhi','india',127),\n",
    "(54,4,'rakesh','2023-08-10','jaipur','india',321),\n",
    "(65,5,'ayush','2023-09-07','mosco','russia',765),\n",
    "(89,6,'rajat','2023-08-10','jaipur','india',321)\n",
    "]\n",
    "\n",
    "sales_schema = ['sales_id', 'customer_id','customer_name', 'sales_date', 'food_delivery_address','food_delivery_country', 'food_cost']\n",
    "\n",
    "sales_df = spark.createDataFrame(data=sales_data,schema=sales_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73f817aa-701b-4d93-843a-42b611f1abcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Join the DF to identify changes in address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fef41f0a-424c-4111-bb87-0b0d3db4ceb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joined_data = customer_dim_df.join(sales_df,customer_dim_df['id']==sales_df['customer_id'],'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "581ef319-12c4-4747-a531-95f91f4a43d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>city</th><th>country</th><th>active</th><th>effective_start_date</th><th>effective_end_date</th><th>sales_id</th><th>customer_id</th><th>customer_name</th><th>sales_date</th><th>food_delivery_address</th><th>food_delivery_country</th><th>food_cost</th></tr></thead><tbody><tr><td>1</td><td>manish</td><td>arwal</td><td>india</td><td>N</td><td>2022-09-15</td><td>2022-09-25</td><td>77</td><td>1</td><td>manish</td><td>2023-03-11</td><td>bangalore</td><td>india</td><td>300</td></tr><tr><td>1</td><td>manish</td><td>arwal</td><td>india</td><td>N</td><td>2022-09-15</td><td>2022-09-25</td><td>1</td><td>1</td><td>manish</td><td>2023-01-16</td><td>gurgaon</td><td>india</td><td>380</td></tr><tr><td>2</td><td>vikash</td><td>patna</td><td>india</td><td>Y</td><td>2023-08-12</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>3</td><td>nikita</td><td>delhi</td><td>india</td><td>Y</td><td>2023-09-10</td><td>null</td><td>12</td><td>3</td><td>nikita</td><td>2023-09-20</td><td>delhi</td><td>india</td><td>127</td></tr><tr><td>4</td><td>rakesh</td><td>jaipur</td><td>india</td><td>Y</td><td>2023-06-10</td><td>null</td><td>54</td><td>4</td><td>rakesh</td><td>2023-08-10</td><td>jaipur</td><td>india</td><td>321</td></tr><tr><td>5</td><td>ayush</td><td>NY</td><td>USA</td><td>Y</td><td>2023-06-10</td><td>null</td><td>65</td><td>5</td><td>ayush</td><td>2023-09-07</td><td>mosco</td><td>russia</td><td>765</td></tr><tr><td>1</td><td>manish</td><td>gurgaon</td><td>india</td><td>Y</td><td>2022-09-25</td><td>null</td><td>77</td><td>1</td><td>manish</td><td>2023-03-11</td><td>bangalore</td><td>india</td><td>300</td></tr><tr><td>1</td><td>manish</td><td>gurgaon</td><td>india</td><td>Y</td><td>2022-09-25</td><td>null</td><td>1</td><td>1</td><td>manish</td><td>2023-01-16</td><td>gurgaon</td><td>india</td><td>380</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "manish",
         "arwal",
         "india",
         "N",
         "2022-09-15",
         "2022-09-25",
         77,
         1,
         "manish",
         "2023-03-11",
         "bangalore",
         "india",
         300
        ],
        [
         1,
         "manish",
         "arwal",
         "india",
         "N",
         "2022-09-15",
         "2022-09-25",
         1,
         1,
         "manish",
         "2023-01-16",
         "gurgaon",
         "india",
         380
        ],
        [
         2,
         "vikash",
         "patna",
         "india",
         "Y",
         "2023-08-12",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         3,
         "nikita",
         "delhi",
         "india",
         "Y",
         "2023-09-10",
         null,
         12,
         3,
         "nikita",
         "2023-09-20",
         "delhi",
         "india",
         127
        ],
        [
         4,
         "rakesh",
         "jaipur",
         "india",
         "Y",
         "2023-06-10",
         null,
         54,
         4,
         "rakesh",
         "2023-08-10",
         "jaipur",
         "india",
         321
        ],
        [
         5,
         "ayush",
         "NY",
         "USA",
         "Y",
         "2023-06-10",
         null,
         65,
         5,
         "ayush",
         "2023-09-07",
         "mosco",
         "russia",
         765
        ],
        [
         1,
         "manish",
         "gurgaon",
         "india",
         "Y",
         "2022-09-25",
         null,
         77,
         1,
         "manish",
         "2023-03-11",
         "bangalore",
         "india",
         300
        ],
        [
         1,
         "manish",
         "gurgaon",
         "india",
         "Y",
         "2022-09-25",
         null,
         1,
         1,
         "manish",
         "2023-01-16",
         "gurgaon",
         "india",
         380
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "active",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "effective_start_date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "effective_end_date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sales_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "customer_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sales_date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "food_delivery_address",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "food_delivery_country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "food_cost",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(joined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dca6de2-ecd7-47e9-8300-004d4fe03dbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---------+---------------------+------+--------------------+------------------+\n|customer_id|customer_name|     city|food_delivery_country|active|effective_start_date|effective_end_date|\n+-----------+-------------+---------+---------------------+------+--------------------+------------------+\n|          1|       manish|bangalore|                india|     Y|          2023-03-11|              null|\n|          5|        ayush|    mosco|               russia|     Y|          2023-09-07|              null|\n+-----------+-------------+---------+---------------------+------+--------------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "new_record_df = joined_data.where(\n",
    "    (col('food_delivery_address') != col('city')) & (col('active') == 'Y'))\\\n",
    "    .withColumn('active',lit('Y'))\\\n",
    "    .withColumn('effective_start_date',col('sales_date'))\\\n",
    "    .withColumn('effective_end_date',lit(None))\\\n",
    "        .select(\n",
    "            'customer_id',\n",
    "            'customer_name',\n",
    "            col('food_delivery_address').alias('city'),\n",
    "            'food_delivery_country',\n",
    "            'active',\n",
    "            'effective_start_date',\n",
    "            'effective_end_date'   \n",
    ")\n",
    "new_record_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a3ce0eb-85e3-45b3-87d6-68eccb654b41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Update Old record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e06e891a-e06b-4ce8-9f5f-ab21e91b57b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------------+------+--------------------+------------------+\n|customer_id|customer_name|   city|food_delivery_country|active|effective_start_date|effective_end_date|\n+-----------+-------------+-------+---------------------+------+--------------------+------------------+\n|          1|       manish|gurgaon|                india|     N|          2022-09-25|        2023-03-11|\n|          5|        ayush|     NY|               russia|     N|          2023-06-10|        2023-09-07|\n+-----------+-------------+-------+---------------------+------+--------------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "old_record_df = joined_data.where(\n",
    "    (col('food_delivery_address') != col('city')) & (col('active') == 'Y'))\\\n",
    "    .withColumn('active',lit('N'))\\\n",
    "    .withColumn('effective_end_date',col('sales_date'))\\\n",
    "        .select(\n",
    "            'customer_id',\n",
    "            'customer_name',\n",
    "            'city',\n",
    "            'food_delivery_country',\n",
    "            'active',\n",
    "            'effective_start_date',\n",
    "            'effective_end_date'   \n",
    ")\n",
    "old_record_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "141b8baa-1986-436b-8d2a-744c6446d7fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Find out new customers and insert them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18051654-90ef-4bc0-bd84-c23022db0ff0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---------------------+---------------------+------+--------------------+------------------+\n|customer_id|customer_name|food_delivery_address|food_delivery_country|active|effective_start_date|effective_end_date|\n+-----------+-------------+---------------------+---------------------+------+--------------------+------------------+\n|          6|        rajat|               jaipur|                india|     Y|          2023-08-10|              null|\n+-----------+-------------+---------------------+---------------------+------+--------------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "new_customers = sales_df.join(\n",
    "  customer_dim_df,sales_df['customer_id']==customer_dim_df['id'],'leftanti'\n",
    ")\\\n",
    "    .withColumn('active',lit('Y'))\\\n",
    "    .withColumn('effective_start_date',col('sales_date'))\\\n",
    "    .withColumn('effective_end_date',lit(None))\\\n",
    "        .select(\n",
    "            'customer_id',\n",
    "            'customer_name',\n",
    "            'food_delivery_address',\n",
    "            'food_delivery_country',\n",
    "            'active',\n",
    "            'effective_start_date',\n",
    "            'effective_end_date'   \n",
    ")\n",
    "new_customers.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e37bc01e-1613-4d84-92e0-c8bd5644203d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Merge all records in one DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d32d3b35-3a66-4cfa-b594-8d4a775b64e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------+-------+------+--------------------+------------------+\n| id|  name|     city|country|active|effective_start_date|effective_end_date|\n+---+------+---------+-------+------+--------------------+------------------+\n|  1|manish|    arwal|  india|     N|          2022-09-15|        2022-09-25|\n|  2|vikash|    patna|  india|     Y|          2023-08-12|              null|\n|  3|nikita|    delhi|  india|     Y|          2023-09-10|              null|\n|  4|rakesh|   jaipur|  india|     Y|          2023-06-10|              null|\n|  5| ayush|       NY|    USA|     Y|          2023-06-10|              null|\n|  1|manish|  gurgaon|  india|     Y|          2022-09-25|              null|\n|  1|manish|bangalore|  india|     Y|          2023-03-11|              null|\n|  5| ayush|    mosco| russia|     Y|          2023-09-07|              null|\n|  1|manish|  gurgaon|  india|     N|          2022-09-25|        2023-03-11|\n|  5| ayush|       NY| russia|     N|          2023-06-10|        2023-09-07|\n|  6| rajat|   jaipur|  india|     Y|          2023-08-10|              null|\n+---+------+---------+-------+------+--------------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "final_records = customer_dim_df.union(new_record_df).union(old_record_df).union(new_customers\n",
    ")\n",
    "final_records.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca7a66eb-ae61-45b5-999d-90f19974a137",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Remove the duplicate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b50cfd4-3157-4f5a-a599-b72071df97c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------+-------+------+--------------------+------------------+\n| id|  name|     city|country|active|effective_start_date|effective_end_date|\n+---+------+---------+-------+------+--------------------+------------------+\n|  1|manish|  gurgaon|  india|     N|          2022-09-25|        2023-03-11|\n|  1|manish|    arwal|  india|     N|          2022-09-15|        2022-09-25|\n|  1|manish|bangalore|  india|     Y|          2023-03-11|              null|\n|  2|vikash|    patna|  india|     Y|          2023-08-12|              null|\n|  3|nikita|    delhi|  india|     Y|          2023-09-10|              null|\n|  4|rakesh|   jaipur|  india|     Y|          2023-06-10|              null|\n|  5| ayush|       NY| russia|     N|          2023-06-10|        2023-09-07|\n|  5| ayush|    mosco| russia|     Y|          2023-09-07|              null|\n|  6| rajat|   jaipur|  india|     Y|          2023-08-10|              null|\n+---+------+---------+-------+------+--------------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "window = Window.partitionBy('id','active').orderBy(col('effective_start_date').desc())\n",
    "\n",
    "final_records.withColumn('rnk',rank().over(window))\\\n",
    "  .filter(~((col('active')=='Y')&(col('rnk')>=2))).drop('rnk').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f13a6e85-a93d-47c3-b79a-1cb10c9d2ab3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 978768651936788,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "SPARK_TUTORIAL_MANISH",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
